{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvrlab/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import re\n",
    "import hickle as hkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.legacy.nn import Reshape\n",
    "import graphviz\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "#from visualize import make_dot\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import time\n",
    "import logging\n",
    "from math import log,sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense201 = models.densenet201(pretrained=True)\n",
    "#dense161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#dense201= nn.Sequential(*list(dense201.children())[:-1])\n",
    "dense201\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input=Variable(torch.randn(1,3,180,320))\n",
    "# output=dense201(input)\n",
    "# print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeWeights(mod):\n",
    "    for m in mod.modules():\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            #print m.weight.size(), m.out_channels, m.in_channels\n",
    "            m.weight.data.normal_(0,sqrt(2./n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.bias.data.zero_()    \n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Sequential(nn.BatchNorm2d(1920),nn.ReLU(),nn.Conv2d(1920,1024,1))\n",
    "conv1 = InitializeWeights(conv1)\n",
    "conv2 = nn.Sequential(nn.BatchNorm2d(1024),nn.ReLU(),nn.Conv2d(1024,128,5))\n",
    "conv2 = InitializeWeights(conv2)\n",
    "conv3 = nn.Sequential(nn.BatchNorm2d(128),nn.ReLU(),nn.Conv2d(128,16,1))\n",
    "conv3 = InitializeWeights(conv3)\n",
    "norm1 = nn.BatchNorm2d(16)\n",
    "norm1 = InitializeWeights(norm1)\n",
    "fc1 = nn.Linear(96, 1)\n",
    "fc1 = InitializeWeights(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel4(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(MyModel4, self).__init__()\n",
    "        self.pretrained_model = nn.Sequential(*list(dense201.children())[:-1])\n",
    "        self.conv1 = conv1\n",
    "        self.conv2 = conv2\n",
    "        self.conv3 = conv3\n",
    "        self.norm1 = norm1\n",
    "        self.fc1 = fc1\n",
    "   \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm1(x)\n",
    "        #print(x.size())\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        #print(x.size())\n",
    "        #x = self.conv4(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyModel4(dense201)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.018627882003784\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "input=Variable(torch.randn(5,3,180,320))\n",
    "tic=time.time()\n",
    "output=net(input)\n",
    "toc=time.time()\n",
    "print(toc-tic)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers --->  618\n",
      "Total number of parameters --->  23345297\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(net.parameters())))\n",
    "for params in net.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(5, 3, 180, 320))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTrainData_59_cor__35000.h5')\n",
    "xtrainT = torch.from_numpy(np.array(file['xtrain'],dtype=np.float32)).float()\n",
    "ytrainT = torch.from_numpy(np.array(file['ytrain'],dtype=np.float32)).float()\n",
    "#xtrain = np.array(file['xtrain'],dtype=np.float32)\n",
    "#ytrain = np.array(file['ytrain'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTestData_22_cor_random_2.h5')\n",
    "xtestT = torch.from_numpy(np.array(file['xtest'],dtype=np.float32)).float()\n",
    "ytestT = torch.from_numpy(np.array(file['ytest'],dtype=np.float32)).float()\n",
    "#xtest = np.array(file['xtest'],dtype=np.float32)\n",
    "#ytest = np.array(file['ytest'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rgb_to_bgr(batch):\n",
    "    #print(batch.size())\n",
    "    (r, g, b) = torch.chunk(batch, 3, 1)\n",
    "    #print(r.size())\n",
    "    batch1 = torch.cat((b, g, r),1)\n",
    "    #print(batch1.size())\n",
    "    return batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = batch_rgb_to_bgr(xtrainT)\n",
    "xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#print(xtrainT.size(), xtestT.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = torch.div(xtrainT,255.0)\n",
    "xtestT = torch.div(xtestT,255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrainT.size(), ytrainT.size(), xtestT.size(), ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"\n",
    "    Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "#xtrainT = norm(xtrainT)\n",
    "xtestT = norm(xtestT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtestT = batch_rgb_to_bgr(xtestT)\n",
    "# xtestT = torch.div(xtestT,255.0)\n",
    "# mn = [0.406,0.456,0.485]\n",
    "# sd = [0.225,0.224,0.229]\n",
    "# norm = Normalize(mn,sd)\n",
    "# xtestT = norm(xtestT)\n",
    "# print(xtestT.size(), ytestT.size())\n",
    "# print(torch.min(xtestT), torch.max(xtestT),torch.min(ytestT), torch.max(ytestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def train(model, loss, optimizer, x_val, y_val, validPixel, batch_sz):\n",
    "def train(model, loss, optimizer, x_val, y_val,batch_size):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    x = batch_rgb_to_bgr(x)\n",
    "    x = torch.div(x,255.0)\n",
    "    mn = [0.406,0.456,0.485]\n",
    "    sd = [0.225,0.224,0.229]\n",
    "    x[:,0,:,:] = (x[:,0,:,:]-mn[0])/sd[0]\n",
    "    x[:,1,:,:] = (x[:,1,:,:]-mn[1])/sd[1]\n",
    "    x[:,2,:,:] = (x[:,2,:,:]-mn[2])/sd[2]\n",
    "    \n",
    "    \n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    #print fx.data[0][0][64][87]\n",
    "    #fx = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "    ##output = loss.forward(fx,y,validPixel,batch_sz)\n",
    "    output = loss.forward(fx,y)\n",
    "    #output = loss(fx, y)\n",
    "    output.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom loss function.... this will be reverse Huber...\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        #k = torch.mean(inp[:,0])\n",
    "        '''\n",
    "        if (k >= 1.48 and k <= 1.65):\n",
    "            diff = torch.abs(tar[:,1]-inp[:,1])\n",
    "            loss = torch.mean(torch.pow(diff,2))\n",
    "        else:\n",
    "        '''\n",
    "        diff = torch.abs(tar[:,0]-inp[:,0]) #*(180/np.pi)\n",
    "        loss = torch.mean(diff)\n",
    "        #print(loss)\n",
    "        return loss\n",
    "        '''\n",
    "        c1 = c.data[0] \n",
    "        temp = diff > c1\n",
    "        check1 = torch.prod(temp)\n",
    "        \n",
    "        if check1 == 0:\n",
    "            lossval = torch.mean(diff)\n",
    "        else:\n",
    "            temp4 = torch.pow(diff,2)\n",
    "            d = torch.pow(c,2)\n",
    "            temp4 = temp4.add(d.expand_as(temp4))\n",
    "            lossval = torch.mean(temp4/(2*c))\n",
    "        return lossval\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class BerhuLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BerhuLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        mt = tar[:,0]\n",
    "        mp = inp[:,0]\n",
    "        diff = torch.abs(mt-mp)        \n",
    "        lossval = 0.0        \n",
    "        c = 0.2 * torch.max(diff)\n",
    "        l1 = torch.mean(diff)\n",
    "        l2 = torch.mean(torch.pow(diff,2))\n",
    "        if l1 <= c:\n",
    "            lossval = l1\n",
    "        else:\n",
    "            lossval = (l2+c**2)/(2*c)\n",
    "        \n",
    "        return lossval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = torch.FloatTensor(ytrainT[5,0])\n",
    "alpha = ytrainT[5,0]\n",
    "#print(alpha.shape)\n",
    "xt = torch.FloatTensor([np.cos(alpha),np.sin(alpha)])\n",
    "print(ytrainT[5,0],xt.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ytrainT[5:10,0]\n",
    "print(torch.cos(alpha[0:1]-alpha[1:2]))\n",
    "xt = torch.stack([torch.cos(alpha[0:1]),torch.sin(alpha[0:1])])\n",
    "xp = torch.stack([torch.cos(alpha[1:2]),torch.sin(alpha[1:2])])\n",
    "print(xt[0],xt[1])\n",
    "#print(los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar,batch_sz):\n",
    "        alpha_t = tar[:,0]\n",
    "        alpha_p = inp[:,0]\n",
    "        #xt = torch.stack([torch.cos(alpha_t),torch.sin(alpha_t)])\n",
    "        #xp = torch.stack([torch.cos(alpha_p),torch.sin(alpha_p)])\n",
    "        #cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        #loss = cos(xt, xp)\n",
    "        #return loss\n",
    "        loss = Variable(torch.FloatTensor(batch_sz).zero_(), requires_grad=False).cuda()\n",
    "        for i in range(batch_sz):          \n",
    "            loss[i] = torch.cos(alpha_t[i:i+1]-alpha_p[i:i+1])\n",
    "            \n",
    "        lossval = 1.0-torch.mean(loss)    \n",
    "        #print(lossval)\n",
    "        return lossval\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.00424446 at epoch 1 completed in 6m 1s\n",
      "Loss 0.004276975539978594 is bigger than Loss 0.004244462707312781 in the prev epoch \n",
      "Loss = 0.00427698 at epoch 2 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00417253 at epoch 3 completed in 5m 60s\n",
      "Loss 0.004226353226229561 is bigger than Loss 0.004172534217359496 in the prev epoch \n",
      "Loss = 0.00422635 at epoch 4 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00416558 at epoch 5 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00414955 at epoch 6 completed in 5m 60s\n",
      "Loss 0.00418763151322492 is bigger than Loss 0.004149546180153265 in the prev epoch \n",
      "Loss = 0.00418763 at epoch 7 completed in 5m 59s\n",
      "Loss 0.004181576702045281 is bigger than Loss 0.004149546180153265 in the prev epoch \n",
      "Loss = 0.00418158 at epoch 8 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00411862 at epoch 9 completed in 5m 60s\n",
      "Loss 0.00420798053056933 is bigger than Loss 0.004118619264801953 in the prev epoch \n",
      "Loss = 0.00420798 at epoch 10 completed in 5m 59s\n",
      "Loss 0.004162973585305734 is bigger than Loss 0.004118619264801953 in the prev epoch \n",
      "Loss = 0.00416297 at epoch 11 completed in 5m 59s\n",
      "Loss 0.004185553219867867 is bigger than Loss 0.004118619264801953 in the prev epoch \n",
      "Loss = 0.00418555 at epoch 12 completed in 5m 59s\n",
      "Loss 0.00416291570314205 is bigger than Loss 0.004118619264801953 in the prev epoch \n",
      "Loss = 0.00416292 at epoch 13 completed in 5m 59s\n",
      "Learning rate changed from 0.0002 to 4e-05\n",
      "Loss 0.004127970750909296 is bigger than Loss 0.004118619264801953 in the prev epoch \n",
      "Loss = 0.00412797 at epoch 14 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00383853 at epoch 15 completed in 5m 59s\n",
      "Loss 0.003848131344653666 is bigger than Loss 0.003838529849424958 in the prev epoch \n",
      "Loss = 0.00384813 at epoch 16 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00381726 at epoch 17 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00377917 at epoch 18 completed in 5m 59s\n",
      "Loss 0.003841611359268427 is bigger than Loss 0.0037791748101590203 in the prev epoch \n",
      "Loss = 0.00384161 at epoch 19 completed in 5m 59s\n",
      "Loss 0.0038058678410015996 is bigger than Loss 0.0037791748101590203 in the prev epoch \n",
      "Loss = 0.00380587 at epoch 20 completed in 5m 59s\n",
      "Loss 0.0038386074313893916 is bigger than Loss 0.0037791748101590203 in the prev epoch \n",
      "Loss = 0.00383861 at epoch 21 completed in 5m 59s\n",
      "Loss 0.003854466243647039 is bigger than Loss 0.0037791748101590203 in the prev epoch \n",
      "Loss = 0.00385447 at epoch 22 completed in 5m 59s\n",
      "Learning rate changed from 4e-05 to 8.000000000000001e-06\n",
      "Loss 0.0038011265569366515 is bigger than Loss 0.0037791748101590203 in the prev epoch \n",
      "Loss = 0.00380113 at epoch 23 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00374374 at epoch 24 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00373261 at epoch 25 completed in 5m 59s\n",
      "Loss 0.0037712183776311575 is bigger than Loss 0.0037326093730516733 in the prev epoch \n",
      "Loss = 0.00377122 at epoch 26 completed in 5m 59s\n",
      "Loss 0.0037417830302147194 is bigger than Loss 0.0037326093730516733 in the prev epoch \n",
      "Loss = 0.00374178 at epoch 27 completed in 5m 59s\n",
      "Loss 0.0037676158589310944 is bigger than Loss 0.0037326093730516733 in the prev epoch \n",
      "Loss = 0.00376762 at epoch 28 completed in 5m 59s\n",
      "Loss 0.0037517094356007874 is bigger than Loss 0.0037326093730516733 in the prev epoch \n",
      "Loss = 0.00375171 at epoch 29 completed in 5m 59s\n",
      "Learning rate changed from 8.000000000000001e-06 to 1.6000000000000004e-06\n",
      "Loss 0.003743456648197025 is bigger than Loss 0.0037326093730516733 in the prev epoch \n",
      "Loss = 0.00374346 at epoch 30 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00372994 at epoch 31 completed in 5m 59s\n",
      "Loss 0.0037948468518443404 is bigger than Loss 0.0037299378104507924 in the prev epoch \n",
      "Loss = 0.00379485 at epoch 32 completed in 5m 59s\n",
      "Loss 0.003751985086244531 is bigger than Loss 0.0037299378104507924 in the prev epoch \n",
      "Loss = 0.00375199 at epoch 33 completed in 5m 59s\n",
      "Loss 0.003732142187887803 is bigger than Loss 0.0037299378104507924 in the prev epoch \n",
      "Loss = 0.00373214 at epoch 34 completed in 5m 59s\n",
      "Loss 0.003737401367397979 is bigger than Loss 0.0037299378104507924 in the prev epoch \n",
      "Loss = 0.00373740 at epoch 35 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00372621 at epoch 36 completed in 5m 59s\n",
      "Loss 0.003734886495163664 is bigger than Loss 0.0037262057738844307 in the prev epoch \n",
      "Loss = 0.00373489 at epoch 37 completed in 5m 59s\n",
      "Loss 0.0037611135351471605 is bigger than Loss 0.0037262057738844307 in the prev epoch \n",
      "Loss = 0.00376111 at epoch 38 completed in 5m 58s\n",
      "save the weights\n",
      "Loss = 0.00371820 at epoch 39 completed in 5m 59s\n",
      "Loss 0.0037450565991457557 is bigger than Loss 0.003718199291732162 in the prev epoch \n",
      "Loss = 0.00374506 at epoch 40 completed in 5m 59s\n",
      "Loss 0.0037397194067016245 is bigger than Loss 0.003718199291732162 in the prev epoch \n",
      "Loss = 0.00373972 at epoch 41 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00371760 at epoch 42 completed in 5m 59s\n",
      "Loss 0.0037559687932953237 is bigger than Loss 0.0037175957224098964 in the prev epoch \n",
      "Loss = 0.00375597 at epoch 43 completed in 5m 60s\n",
      "Loss 0.003782290837727487 is bigger than Loss 0.0037175957224098964 in the prev epoch \n",
      "Loss = 0.00378229 at epoch 44 completed in 5m 59s\n",
      "Loss 0.0037402794631198047 is bigger than Loss 0.0037175957224098964 in the prev epoch \n",
      "Loss = 0.00374028 at epoch 45 completed in 5m 59s\n",
      "Loss 0.0037529248361242934 is bigger than Loss 0.0037175957224098964 in the prev epoch \n",
      "Loss = 0.00375292 at epoch 46 completed in 5m 59s\n",
      "Learning rate changed from 1.6000000000000004e-06 to 3.2000000000000006e-07\n",
      "Loss 0.003741330549004488 is bigger than Loss 0.0037175957224098964 in the prev epoch \n",
      "Loss = 0.00374133 at epoch 47 completed in 5m 59s\n",
      "Loss 0.003733976295217872 is bigger than Loss 0.0037175957224098964 in the prev epoch \n",
      "Loss = 0.00373398 at epoch 48 completed in 5m 59s\n",
      "Loss 0.0037202238398604094 is bigger than Loss 0.0037175957224098964 in the prev epoch \n",
      "Loss = 0.00372022 at epoch 49 completed in 5m 59s\n",
      "Loss 0.0037504211394116284 is bigger than Loss 0.0037175957224098964 in the prev epoch \n",
      "Loss = 0.00375042 at epoch 50 completed in 5m 59s\n",
      "Loss 0.003768092474201694 is bigger than Loss 0.0037175957224098964 in the prev epoch \n",
      "Loss = 0.00376809 at epoch 51 completed in 5m 59s\n",
      "Learning rate changed from 3.2000000000000006e-07 to 6.400000000000002e-08\n",
      "Loss 0.0037411514013074337 is bigger than Loss 0.0037175957224098964 in the prev epoch \n",
      "Loss = 0.00374115 at epoch 52 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00371487 at epoch 53 completed in 5m 59s\n",
      "Loss 0.003729595441604033 is bigger than Loss 0.003714865834917873 in the prev epoch \n",
      "Loss = 0.00372960 at epoch 54 completed in 5m 59s\n",
      "Loss 0.0037310979001922533 is bigger than Loss 0.003714865834917873 in the prev epoch \n",
      "Loss = 0.00373110 at epoch 55 completed in 5m 59s\n",
      "Loss 0.003722834844375029 is bigger than Loss 0.003714865834917873 in the prev epoch \n",
      "Loss = 0.00372283 at epoch 56 completed in 5m 59s\n",
      "save the weights\n",
      "Loss = 0.00370624 at epoch 57 completed in 5m 59s\n",
      "Loss 0.003710179990390316 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00371018 at epoch 58 completed in 5m 59s\n",
      "Loss 0.0037611096159089357 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00376111 at epoch 59 completed in 5m 59s\n",
      "Loss 0.003719768049311824 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00371977 at epoch 60 completed in 5m 59s\n",
      "Loss 0.00371300025167875 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00371300 at epoch 61 completed in 5m 60s\n",
      "Learning rate changed from 6.400000000000002e-08 to 1.2800000000000004e-08\n",
      "Loss 0.003741187166189775 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00374119 at epoch 62 completed in 5m 60s\n",
      "Loss 0.0037754016234539446 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00377540 at epoch 63 completed in 5m 60s\n",
      "Loss 0.003764286106452346 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00376429 at epoch 64 completed in 5m 60s\n",
      "Loss 0.0037247181453276424 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00372472 at epoch 65 completed in 5m 60s\n",
      "Loss 0.003707752679474652 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00370775 at epoch 66 completed in 5m 60s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate changed from 1.2800000000000004e-08 to 2.5600000000000007e-09\n",
      "Loss 0.0037256659888662397 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00372567 at epoch 67 completed in 5m 60s\n",
      "Loss 0.003720375417266041 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00372038 at epoch 68 completed in 5m 60s\n",
      "Loss 0.0037173968218266965 is bigger than Loss 0.0037062371474457906 in the prev epoch \n",
      "Loss = 0.00371740 at epoch 69 completed in 5m 60s\n",
      "save the weights\n",
      "Loss = 0.00370133 at epoch 70 completed in 5m 60s\n",
      "Loss 0.003717429761774838 is bigger than Loss 0.003701334331533872 in the prev epoch \n",
      "Loss = 0.00371743 at epoch 71 completed in 5m 60s\n",
      "Loss 0.0037319197345059363 is bigger than Loss 0.003701334331533872 in the prev epoch \n",
      "Loss = 0.00373192 at epoch 72 completed in 5m 60s\n",
      "Loss 0.0037138243999797855 is bigger than Loss 0.003701334331533872 in the prev epoch \n",
      "Loss = 0.00371382 at epoch 73 completed in 5m 60s\n",
      "Loss 0.0037295272352639588 is bigger than Loss 0.003701334331533872 in the prev epoch \n",
      "Loss = 0.00372953 at epoch 74 completed in 5m 60s\n",
      "Learning rate changed from 2.5600000000000007e-09 to 5.120000000000001e-10\n",
      "Loss 0.0038007546968292444 is bigger than Loss 0.003701334331533872 in the prev epoch \n",
      "Loss = 0.00380075 at epoch 75 completed in 5m 60s\n",
      "Loss 0.0037079899224918334 is bigger than Loss 0.003701334331533872 in the prev epoch \n",
      "Loss = 0.00370799 at epoch 76 completed in 5m 60s\n",
      "Loss 0.0038024733408819882 is bigger than Loss 0.003701334331533872 in the prev epoch \n",
      "Loss = 0.00380247 at epoch 77 completed in 5m 60s\n",
      "Loss 0.003741413228912279 is bigger than Loss 0.003701334331533872 in the prev epoch \n",
      "Loss = 0.00374141 at epoch 78 completed in 5m 60s\n",
      "Loss 0.0037198262377642097 is bigger than Loss 0.003701334331533872 in the prev epoch \n",
      "Loss = 0.00371983 at epoch 79 completed in 5m 60s\n",
      "Learning rate changed from 5.120000000000001e-10 to 1.0240000000000002e-10\n",
      "Loss 0.003716520585352555 is bigger than Loss 0.003701334331533872 in the prev epoch \n",
      "Loss = 0.00371652 at epoch 80 completed in 5m 60s\n",
      "Loss 0.003755599540891126 is bigger than Loss 0.003701334331533872 in the prev epoch \n",
      "Loss = 0.00375560 at epoch 81 completed in 5m 60s\n",
      "save the weights\n",
      "Loss = 0.00369576 at epoch 82 completed in 5m 60s\n",
      "Loss 0.003743526289705187 is bigger than Loss 0.003695759948575869 in the prev epoch \n",
      "Loss = 0.00374353 at epoch 83 completed in 5m 60s\n",
      "Loss 0.003760649770498276 is bigger than Loss 0.003695759948575869 in the prev epoch \n",
      "Loss = 0.00376065 at epoch 84 completed in 5m 60s\n",
      "Loss 0.0037598939007148146 is bigger than Loss 0.003695759948575869 in the prev epoch \n",
      "Loss = 0.00375989 at epoch 85 completed in 5m 60s\n",
      "Loss 0.0037460431281942873 is bigger than Loss 0.003695759948575869 in the prev epoch \n",
      "Loss = 0.00374604 at epoch 86 completed in 5m 60s\n",
      "Learning rate changed from 1.0240000000000002e-10 to 2.0480000000000003e-11\n",
      "Loss 0.003711663139984012 is bigger than Loss 0.003695759948575869 in the prev epoch \n",
      "Loss = 0.00371166 at epoch 87 completed in 5m 60s\n",
      "Loss 0.0037327007959829643 is bigger than Loss 0.003695759948575869 in the prev epoch \n",
      "Loss = 0.00373270 at epoch 88 completed in 5m 60s\n",
      "Loss 0.0037351946218404917 is bigger than Loss 0.003695759948575869 in the prev epoch \n",
      "Loss = 0.00373519 at epoch 89 completed in 5m 60s\n",
      "Loss 0.003803920998936519 is bigger than Loss 0.003695759948575869 in the prev epoch \n",
      "Loss = 0.00380392 at epoch 90 completed in 5m 60s\n",
      "Loss 0.0037328062562737613 is bigger than Loss 0.003695759948575869 in the prev epoch \n",
      "Loss = 0.00373281 at epoch 91 completed in 5m 60s\n",
      "save the weights\n",
      "Loss = 0.00367186 at epoch 92 completed in 5m 60s\n",
      "Loss 0.003751658533234149 is bigger than Loss 0.0036718640164472164 in the prev epoch \n",
      "Loss = 0.00375166 at epoch 93 completed in 5m 60s\n",
      "Loss 0.0037308053247397765 is bigger than Loss 0.0036718640164472164 in the prev epoch \n",
      "Loss = 0.00373081 at epoch 94 completed in 5m 60s\n",
      "Loss 0.0037220699646277355 is bigger than Loss 0.0036718640164472164 in the prev epoch \n",
      "Loss = 0.00372207 at epoch 95 completed in 5m 60s\n",
      "Loss 0.0037635657535865902 is bigger than Loss 0.0036718640164472164 in the prev epoch \n",
      "Loss = 0.00376357 at epoch 96 completed in 5m 60s\n",
      "Learning rate changed from 2.0480000000000003e-11 to 4.096e-12\n",
      "Loss 0.003731633226270788 is bigger than Loss 0.0036718640164472164 in the prev epoch \n",
      "Loss = 0.00373163 at epoch 97 completed in 5m 60s\n",
      "Loss 0.0036907789250835775 is bigger than Loss 0.0036718640164472164 in the prev epoch \n",
      "Loss = 0.00369078 at epoch 98 completed in 5m 60s\n",
      "Loss 0.0037206470007076857 is bigger than Loss 0.0036718640164472164 in the prev epoch \n",
      "Loss = 0.00372065 at epoch 99 completed in 5m 60s\n",
      "Loss 0.003759212611243129 is bigger than Loss 0.0036718640164472164 in the prev epoch \n",
      "Loss = 0.00375921 at epoch 100 completed in 5m 60s\n"
     ]
    }
   ],
   "source": [
    "#MUST UNCOMMENT BELOW LINE...\n",
    "    \n",
    "net = net.cuda()\n",
    "\n",
    "#loading the model after the weights of epoch50.. to check what loss the model gives if lr is taken as 0.0001\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.0002, momentum=0.9)\n",
    "\n",
    "#criterion = RMSELoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = EuclideanLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = CosineLoss()\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "criterion = CustomLoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = CosineLoss()\n",
    "criterion.cuda()\n",
    "\n",
    "currepochloss =0.00428612#float('Inf')\n",
    "#epochs, n_examples, i, batch_size, flag = 1,5900, 0, 5, 0\n",
    "epochs, n_examples, i, batch_size, flag = 100, 35000, 0, 35, 0\n",
    "\n",
    "\n",
    "while i != epochs:\n",
    "    since = time.time()\n",
    "    cost, batchloss = 0.0, 0.0\n",
    "    num_batches = n_examples//batch_size\n",
    "    #print num_batches    #indices = np.random.permutation(5600)\n",
    "    #indices = np.random.permutation(3524)\n",
    "    \n",
    "    #indices = np.random.permutation(5900)\n",
    "    indices = np.random.permutation(n_examples)\n",
    "    samplesUnprocessed = np.size(indices)\n",
    "    \n",
    "    #batchwise training starts here...\n",
    "    for k in range(num_batches):\n",
    "        since1 = time.time()\n",
    "       # print(\"bacth number:\"+str(k))\n",
    "        xtrain3 = torch.FloatTensor(batch_size,3,180,320)\n",
    "        ytrain3 = torch.FloatTensor(batch_size,1)\n",
    "        ##validPixel = torch.FloatTensor(batch_size,480,640)\n",
    "        \n",
    "        for ind in range(batch_size):\n",
    "            #ind1 = np.random.randint(0,5599)\n",
    "            ind1 = np.random.randint(0,samplesUnprocessed)\n",
    "            #ind1 = np.random.randint(0,794)\n",
    "            #ind1 = np.random.randint(0,794)            \n",
    "            newxind = indices[ind1]            \n",
    "            xtrain3[ind] = xtrainT[newxind]\n",
    "            ytrain3[ind] = ytrainT[newxind,0,0]\n",
    "            ##validPixel[ind] = imgValidTrain2[newxind]\n",
    "            \n",
    "            #print ytrain3[ind,0,0,0], ytrain2[newxind,0,0,0]\n",
    "            indices = np.delete(indices,ind1)\n",
    "            samplesUnprocessed = samplesUnprocessed - 1\n",
    "        \n",
    "        #start, end = k*batch_size, (k+1)*batch_size\n",
    "        #batchloss = train(model5,criterion, optimizer, xtrain3, ytrain3, validPixel,batch_size)\n",
    "        batchloss = train(net,criterion, optimizer, xtrain3, ytrain3, batch_size)\n",
    "        batch_time = time.time() - since1\n",
    "        #cost += batchloss\n",
    "        cost = (cost*k+batchloss)/(k+1)\n",
    "        #print k,cost\n",
    "        #print(\"No. of samples UnProcessed \"+str(samplesUnprocessed))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    epochloss = cost #/num_batches\n",
    "    \n",
    "    if epochloss < currepochloss:\n",
    "        print('save the weights')\n",
    "        torch.save(net.state_dict(),\"./weights/CustomLoss_new/DENSE_NET_201_CustomLoss_new_35000_ANGLE_110+10+10+110_epochs.pth\")\n",
    "        flag = 0\n",
    "        currepochloss = epochloss\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "        if flag == 5:\n",
    "            for p in optimizer.param_groups:\n",
    "                lr2 = p['lr']\n",
    "            newlr = lr2/5\n",
    "            \n",
    "            if newlr < 1e-15:\n",
    "                print(\"Cant decrease further!!\")\n",
    "                newlr = 1e-15\n",
    "            flag = 0 \n",
    "            optimizer = optim.SGD(net.parameters(), lr=newlr, momentum=0.9)\n",
    "            print(\"Learning rate changed from \"+str(lr2)+\" to \"+str(newlr))\n",
    "            \n",
    "        print(\"Loss \"+str(epochloss)+\" is bigger than Loss \"+str(currepochloss)+\" in the prev epoch \")\n",
    "        \n",
    "    print('Loss = {:.8f} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(epochloss,(i+1),(time_elapsed//60),(time_elapsed%60)))\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.096e-12\n"
     ]
    }
   ],
   "source": [
    "for params in optimizer.param_groups:\n",
    "    print(params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()\n",
    "net.load_state_dict(torch.load(\"./weights/CustomLoss_new/DENSE_NET_201_CustomLoss_new_35000_ANGLE_110+10+10+110_epochs.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalpred size is --->  torch.Size([600, 1])\n",
      "num of batches ---> 30\n"
     ]
    }
   ],
   "source": [
    "#testing of the architecture...\n",
    "num_batches = 0\n",
    "#6 evenly divides the test batch size..\n",
    "test_batch_size = 20\n",
    "n_examples = 600\n",
    "#finalpred = Variable(torch.zeros((n_examples,3,120,160)))\n",
    "finalpred = Variable(torch.zeros((n_examples,1)))\n",
    "print(\"finalpred size is ---> \", finalpred.size())\n",
    "\n",
    "num_batches = n_examples//test_batch_size\n",
    "print(\"num of batches --->\", num_batches)\n",
    "for k in range(num_batches):\n",
    "    start, end = k*test_batch_size, (k+1)*test_batch_size\n",
    "    output = net.forward(Variable(xtestT[start:end], volatile=True).cuda())\n",
    "    finalpred[start:end] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = finalpred.data.numpy()\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600])\n",
      "MSElossRad==0.002161332594135175 ABSlossRad==0.0312666854262352 RELlossRad0.024907450231900535\n",
      "MSElossDeg==0.12383523576800722 ABSlossDeg==1.7914491142864766 RELlossDeg1.4270917767200442\n"
     ]
    }
   ],
   "source": [
    "#---------------ANGLE------------------------\n",
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,0,0])\n",
    "dif1 = torch.abs((finalpred.data[:,0]-ytestT[:,0,0])/ytestT[:,0,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "\n",
    "MSElossRad = torch.mean(torch.pow(dif,2))\n",
    "ABSlossRad = torch.mean(dif)\n",
    "RELlossRad = torch.mean(dif1)\n",
    "MSElossDeg = MSElossRad*(180/np.pi)\n",
    "ABSlossDeg = ABSlossRad*(180/np.pi)\n",
    "RELlossDeg = RELlossRad*(180/np.pi)\n",
    "print(\"MSElossRad==\"+str(MSElossRad),\"ABSlossRad==\"+str(ABSlossRad),\"RELlossRad\"+str(RELlossRad))\n",
    "print(\"MSElossDeg==\"+str(MSElossDeg),\"ABSlossDeg==\"+str(ABSlossDeg),\"RELlossDeg\"+str(RELlossDeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------DISTANCE------------------------\n",
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,1,0])\n",
    "dif1 = torch.abs((finalpred.data[:,0]-ytestT[:,1,0])/ytestT[:,1,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "\n",
    "MSElossNor = torch.mean(torch.pow(dif,2))\n",
    "ABSlossNor = torch.mean(dif)\n",
    "RELlossNor = torch.mean(dif1)\n",
    "MSEloss = MSElossNor*320\n",
    "ABSloss = ABSlossNor*320\n",
    "RELloss = RELlossNor*320\n",
    "print(\"MSElossNor==\"+str(MSElossNor),\"ABSlossNor==\"+str(ABSlossNor),\"RELlossNor\"+str(RELlossNor))\n",
    "print(\"MSEloss==\"+str(MSEloss),\"ABSloss==\"+str(ABSloss),\"RELloss\"+str(RELloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finalpred.size())\n",
    "print(ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "ind = 143\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0,0]*(180/np.pi)))\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Pred angle===\"+str(finalpred.data[ind,0]*(180/np.pi)))\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "#print(finalpred.data[n,0]*(180/np.pi))\n",
    "#print(ytestT[ind,0,0]*(180/np.pi))\n",
    "print(ytestT[ind,0,0]*(180/np.pi)-finalpred.data[ind,0]*(180/np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ABSlossRad*(180/np.pi))\n",
    "print(MSEloss*(180/np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(ytestT[:,0]*(180/np.pi))\n",
    "#print(ytestT[:,0,2]*(180/np.pi))\n",
    "a = ytestT[:,0,0]*(180/np.pi)\n",
    "print(a.size())\n",
    "np.savetxt(\"test.csv\", a.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "#np.savetxt(\"foo.csv\", a, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(finalpred.data[:,0]*(180/np.pi))\n",
    "print(finalpred.data[:,0]*(180/np.pi))\n",
    "b = finalpred.data[:,0]*(180/np.pi)\n",
    "c=torch.abs(ytestT[:,0,0]*(180/np.pi)- finalpred.data[:,0]*(180/np.pi))\n",
    "np.savetxt(\"pred.csv\", b.numpy(), delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"diff.csv\", c.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpred.data[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "test = cv2.imread(\"./Test_Net_image/4.JPG\")\n",
    "print(test.shape)\n",
    "test = imresize(test,(180,320,3))\n",
    "#imshow(test)\n",
    "test = test.transpose(2,0,1)\n",
    "test = np.reshape(test,(1,3,180,320))\n",
    "test = test.astype(np.float32)\n",
    "testPT = torch.from_numpy(test).float()\n",
    "testPT = batch_rgb_to_bgr(testPT)\n",
    "testPT = torch.div(testPT,255.0)\n",
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "testPT = norm(testPT)\n",
    "'''\n",
    "ind = 2000\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0]*(180/np.pi)), ytestT[ind,0])\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "#'''\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Angle===\"+str(test_pred.data[0,0]*(180/np.pi)), test_pred.data[0,0])\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
