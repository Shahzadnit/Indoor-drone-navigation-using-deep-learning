{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import re\n",
    "#import hickle as hkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.legacy.nn import Reshape\n",
    "import graphviz\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "#from visualize import make_dot\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import time\n",
    "import logging\n",
    "from math import log,sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "incep_v3 = models.inception_v3(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(incep_v3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[InceptionD(\n",
       "   (branch3x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3_2): BasicConv2d(\n",
       "     (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch7x7x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch7x7x3_2): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch7x7x3_3): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch7x7x3_4): BasicConv2d(\n",
       "     (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       " ), InceptionE(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3_2a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3_2b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3dbl_3a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3dbl_3b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       " ), InceptionE(\n",
       "   (branch1x1): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3_1): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3_2a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3_2b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3dbl_1): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3dbl_2): BasicConv2d(\n",
       "     (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3dbl_3a): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch3x3dbl_3b): BasicConv2d(\n",
       "     (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
       "     (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       "   (branch_pool): BasicConv2d(\n",
       "     (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "     (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
       "   )\n",
       " )]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#incep_v3.AuxLogits\n",
    "list(incep_v3.children())[14:17]\n",
    "\n",
    "#list(incep_v3.children())[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeWeights(mod):\n",
    "    for m in mod.modules():\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            #print m.weight.size(), m.out_channels, m.in_channels\n",
    "            m.weight.data.normal_(0,sqrt(2./n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.bias.data.zero_()\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_l = nn.Sequential(nn.BatchNorm2d(2048),nn.ReLU(),nn.Conv2d(2048,1024,1))\n",
    "conv1_l = InitializeWeights(conv1_l)\n",
    "conv2_l = nn.Sequential(nn.BatchNorm2d(1024),nn.ReLU(),nn.Conv2d(1024,512,(2,2),(1,2)))\n",
    "conv2_l = InitializeWeights(conv2_l)\n",
    "conv3_l = nn.Sequential(nn.BatchNorm2d(512),nn.ReLU(),nn.Conv2d(512,128,(3,3)))\n",
    "conv3_l = InitializeWeights(conv3_l)\n",
    "norm1_l = nn.BatchNorm2d(128)\n",
    "norm1_l = InitializeWeights(norm1_l)\n",
    "fc1_l = nn.Sequential(nn.Linear(256, 1))\n",
    "fc1_l = InitializeWeights(fc1_l)\n",
    "#############__________Aux_____________#############\n",
    "conv1_aux = nn.Sequential(nn.BatchNorm2d(768),nn.ReLU(),nn.Conv2d(768,128,4,(1,2),(1,0)))\n",
    "conv1_aux = InitializeWeights(conv1_aux)\n",
    "conv2_aux = nn.Sequential(nn.BatchNorm2d(128),nn.ReLU(),nn.Conv2d(128,32,(1,2)))\n",
    "conv2_aux = InitializeWeights(conv2_aux)\n",
    "norm1_aux = nn.BatchNorm2d(32)\n",
    "norm1_aux = InitializeWeights(norm1_aux)\n",
    "fc1_aux = nn.Sequential(nn.Linear(640, 1))\n",
    "fc1_aux = InitializeWeights(fc1_aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IV3_git(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(IV3_git, self).__init__()\n",
    "        self.layer0_2 = nn.Sequential(*list(incep_v3.children())[0:3])#[0,1,2]\n",
    "        self.layer3_4 =  nn.Sequential(*list(incep_v3.children())[3:5])#[3,4]\n",
    "        self.layer5_12 =  nn.Sequential(*list(incep_v3.children())[5:13])#[5,12]\n",
    "        self.layer14_16 =  nn.Sequential(*list(incep_v3.children())[14:17])#[14,6]\n",
    "        ############\n",
    "        self.aauxLogits = nn.Sequential(*list(incep_v3.AuxLogits.children())[:-1])\n",
    "        #######################\n",
    "        self.conv1_l = conv1_l\n",
    "        self.conv2_l = conv2_l\n",
    "        self.conv3_l = conv3_l\n",
    "        self.norm1_l = norm1_l\n",
    "        self.fc1_l = fc1_l\n",
    "        ########################   \n",
    "        self.conv1_aux = conv1_aux\n",
    "        self.conv2_aux = conv2_aux\n",
    "        #self.conv3_aux = conv3_aux\n",
    "        self.norm1_aux = norm1_aux\n",
    "        self.fc1_aux= fc1_aux\n",
    "        \n",
    "        \n",
    "            \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features        \n",
    "            \n",
    "    def forward(self, x):\n",
    "        y1 = self.layer0_2(x)\n",
    "        y1 = F.max_pool2d(y1, kernel_size=3, stride=2)\n",
    "        y1 = self.layer3_4(y1)\n",
    "        y1 = F.max_pool2d(y1, kernel_size=3, stride=2)\n",
    "        y1 = self.layer5_12(y1)\n",
    "        #################\n",
    "        #if self.aux_logits:\n",
    "        aux = self.aauxLogits(y1)\n",
    "        aux = self.conv1_aux(aux)\n",
    "        aux = self.conv2_aux(aux)\n",
    "        #aux = self.conv3_aux(aux)\n",
    "        aux = self.norm1_aux(aux)\n",
    "        aux = aux.view(-1, self.num_flat_features(aux))\n",
    "        aux = self.fc1_aux(aux)\n",
    "        ##################\n",
    "        y1 = self.layer14_16(y1)\n",
    "        #y1 = F.avg_pool2d(y1, kernel_size=8)\n",
    "        y1 = F.dropout(y1)\n",
    "        y1 = self.conv1_l(y1)\n",
    "        y1 = self.conv2_l(y1)\n",
    "        y1 = self.conv3_l(y1)\n",
    "        y1 = self.norm1_l(y1)\n",
    "        y1 = y1.view(-1, self.num_flat_features(y1))\n",
    "        y1= self.fc1_l(y1)\n",
    "        ##################\n",
    "        \n",
    "        return aux,y1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = IV3_git(incep_v3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IV3_git(\n",
      "  (layer0_2): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (2): BasicConv2d(\n",
      "      (conv): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3_4): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(64, 80, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(80, eps=0.001, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(80, 192, kernel_size=(3, 3), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layer5_12): Sequential(\n",
      "    (0): InceptionA(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch5x5_1): BasicConv2d(\n",
      "        (conv): Conv2d(192, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch5x5_2): BasicConv2d(\n",
      "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(32, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): InceptionA(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch5x5_1): BasicConv2d(\n",
      "        (conv): Conv2d(256, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch5x5_2): BasicConv2d(\n",
      "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InceptionA(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch5x5_1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch5x5_2): BasicConv2d(\n",
      "        (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (3): InceptionB(\n",
      "      (branch3x3): BasicConv2d(\n",
      "        (conv): Conv2d(288, 384, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(288, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (4): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(128, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(128, 128, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(128, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (5): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (6): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 160, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(160, 160, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(160, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(160, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (7): InceptionC(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7_3): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_3): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_4): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7dbl_5): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer14_16): Sequential(\n",
      "    (0): InceptionD(\n",
      "      (branch3x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 320, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7x3_2): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(1, 7), stride=(1, 1), padding=(0, 3), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7x3_3): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(7, 1), stride=(1, 1), padding=(3, 0), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch7x7x3_4): BasicConv2d(\n",
      "        (conv): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (1): InceptionE(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3_2a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3_2b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_3a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_3b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(1280, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "    (2): InceptionE(\n",
      "      (branch1x1): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 320, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(320, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3_1): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3_2a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3_2b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_1): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 448, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(448, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_2): BasicConv2d(\n",
      "        (conv): Conv2d(448, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_3a): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch3x3dbl_3b): BasicConv2d(\n",
      "        (conv): Conv2d(384, 384, kernel_size=(3, 1), stride=(1, 1), padding=(1, 0), bias=False)\n",
      "        (bn): BatchNorm2d(384, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "      (branch_pool): BasicConv2d(\n",
      "        (conv): Conv2d(2048, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (bn): BatchNorm2d(192, eps=0.001, momentum=0.1, affine=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (aauxLogits): Sequential(\n",
      "    (0): BasicConv2d(\n",
      "      (conv): Conv2d(768, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(128, eps=0.001, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (1): BasicConv2d(\n",
      "      (conv): Conv2d(128, 768, kernel_size=(5, 5), stride=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(768, eps=0.001, momentum=0.1, affine=True)\n",
      "    )\n",
      "  )\n",
      "  (conv1_l): Sequential(\n",
      "    (0): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (conv2_l): Sequential(\n",
      "    (0): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(1024, 512, kernel_size=(2, 2), stride=(1, 2))\n",
      "  )\n",
      "  (conv3_l): Sequential(\n",
      "    (0): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(512, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (norm1_l): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc1_l): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "  (conv1_aux): Sequential(\n",
      "    (0): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(768, 128, kernel_size=(4, 4), stride=(1, 2), padding=(1, 0))\n",
      "  )\n",
      "  (conv2_aux): Sequential(\n",
      "    (0): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(128, 32, kernel_size=(1, 2), stride=(1, 1))\n",
      "  )\n",
      "  (norm1_aux): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (fc1_aux): Sequential(\n",
      "    (0): Linear(in_features=640, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aux\t torch.Size([10, 1])\n",
      "incep\t torch.Size([10, 1])\n",
      "4.10809326171875\n"
     ]
    }
   ],
   "source": [
    "input1=Variable(torch.Tensor(10,3,180,320))\n",
    "tic=time.time()\n",
    "aux,incep=net(input1)\n",
    "tac=time.time()\n",
    "print(\"aux\\t \"+str(aux.size()))\n",
    "print(\"incep\\t \"+str(incep.size()))\n",
    "print(tac-tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 0\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "Variable containing:\n",
      "1.00000e-08 *\n",
      "  7.0211\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(incep)\n",
    "print(aux)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers --->  292\n",
      "Total number of parameters --->  27161264\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(incep_v3.parameters())))\n",
    "for params in incep_v3.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of layers --->  316\n",
      "Total number of parameters --->  30720450\n"
     ]
    }
   ],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(net.parameters())))\n",
    "for params in net.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #############      rough\n",
    "# ind1 = np.random.randint(0,24)\n",
    "# print(ind1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188.60900568962097\n"
     ]
    }
   ],
   "source": [
    "tic_1=time.time()\n",
    "file = h5py.File('./DATASET/CODE/NewTrainData_59_cor__35000.h5')\n",
    "xtrainT = torch.from_numpy(np.array(file['xtrain'],dtype=np.float32)).float()\n",
    "ytrainT = torch.from_numpy(np.array(file['ytrain'],dtype=np.float32)).float()\n",
    "#xtrain = np.array(file['xtrain'],dtype=np.float32)\n",
    "#ytrain = np.array(file['ytrain'],dtype=np.float32)\n",
    "toc_1=time.time()\n",
    "print(toc_1-tic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTestData_22_cor_random_2.h5')\n",
    "xtestT = torch.from_numpy(np.array(file['xtest'],dtype=np.float32)).float()\n",
    "ytestT = torch.from_numpy(np.array(file['ytest'],dtype=np.float32)).float()\n",
    "#xtest = np.array(file['xtest'],dtype=np.float32)\n",
    "#ytest = np.array(file['ytest'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rgb_to_bgr(batch):\n",
    "    #print(batch.size())\n",
    "    (r, g, b) = torch.chunk(batch, 3, 1)\n",
    "    #print(r.size())\n",
    "    batch1 = torch.cat((b, g, r),1)\n",
    "    #print(batch1.size())\n",
    "    return batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35000, 3, 180, 320])\n"
     ]
    }
   ],
   "source": [
    "#xtrainT = batch_rgb_to_bgr(xtrainT)\n",
    "xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#print(xtrainT.size(), xtestT.size())\n",
    "#print(xtrainT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = torch.div(xtrainT,255.0)\n",
    "xtestT = torch.div(xtestT,255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denom = [255.0,255.0,255.0]\n",
    "# for t, m in zip(xtrainT, denom):\n",
    "#          t.div_(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([35000, 3, 180, 320]) torch.Size([35000, 2, 1]) torch.Size([600, 3, 180, 320]) torch.Size([600, 2, 1])\n"
     ]
    }
   ],
   "source": [
    "print(xtrainT.size(), ytrainT.size(), xtestT.size(), ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"\n",
    "    Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "#xtrainT = norm(xtrainT)\n",
    "xtestT = norm(xtestT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))\n",
    "print(torch.min(xtrainT), torch.max(xtrainT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrainT[1,0,0,0])\n",
    "type(xtrainT)\n",
    "#xtrainT.index(255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loss, optimizer, x_val, y_val,batch_size):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    x = batch_rgb_to_bgr(x)\n",
    "    x = torch.div(x,255.0)\n",
    "    mn = [0.406,0.456,0.485]\n",
    "    sd = [0.225,0.224,0.229]\n",
    "    x[:,0,:,:] = (x[:,0,:,:]-mn[0])/sd[0]\n",
    "    x[:,1,:,:] = (x[:,1,:,:]-mn[1])/sd[1]\n",
    "    x[:,2,:,:] = (x[:,2,:,:]-mn[2])/sd[2]\n",
    "    #x=torch.div(torch.sub(x-mn),sd)\n",
    "    \n",
    "    fx1,fx2 = model.forward(x)\n",
    "    \n",
    "    #print fx.data[0][0][64][87]\n",
    "    #fx = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "    ##output = loss.forward(fx,y,validPixel,batch_sz)\n",
    "    output = loss.forward(fx1,fx2,y)\n",
    "    #output = loss(fx, y)\n",
    "    output.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp1,inp2, tar):\n",
    "        #target is the ground truth value...\n",
    "        #k = torch.mean(inp[:,0])\n",
    "        '''\n",
    "        if (k >= 1.48 and k <= 1.65):\n",
    "            diff = torch.abs(tar[:,1]-inp[:,1])\n",
    "            loss = torch.mean(torch.pow(diff,2))\n",
    "        else:\n",
    "        '''\n",
    "        diff1 = torch.abs(tar[:,0]-inp1[:,0]) #*(180/np.pi)\n",
    "        diff2 = torch.abs(tar[:,0]-inp2[:,0])\n",
    "        diff = diff1+diff2\n",
    "        loss = torch.mean(diff)\n",
    "        #print(loss)\n",
    "        return loss\n",
    "        '''\n",
    "        c1 = c.data[0] \n",
    "        temp = diff > c1\n",
    "        check1 = torch.prod(temp)\n",
    "        \n",
    "        if check1 == 0:\n",
    "            lossval = torch.mean(diff)\n",
    "        else:\n",
    "            temp4 = torch.pow(diff,2)\n",
    "            d = torch.pow(c,2)\n",
    "            temp4 = temp4.add(d.expand_as(temp4))\n",
    "            lossval = torch.mean(temp4/(2*c))\n",
    "        return lossval\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0400802151166967 is bigger than Loss 0.03734306 in the prev epoch \n",
      "Loss = 0.04008022 at epoch 1 completed in 3m 51s\n",
      "Loss 0.038256109897047315 is bigger than Loss 0.03734306 in the prev epoch \n",
      "Loss = 0.03825611 at epoch 2 completed in 3m 51s\n",
      "Loss 0.03895939745541125 is bigger than Loss 0.03734306 in the prev epoch \n",
      "Loss = 0.03895940 at epoch 3 completed in 3m 51s\n",
      "Loss 0.03899936936795712 is bigger than Loss 0.03734306 in the prev epoch \n",
      "Loss = 0.03899937 at epoch 4 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.03589356 at epoch 5 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.03514726 at epoch 6 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.03444020 at epoch 7 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.03358500 at epoch 8 completed in 3m 51s\n",
      "Loss 0.03421180529253828 is bigger than Loss 0.033584998630519385 in the prev epoch \n",
      "Loss = 0.03421181 at epoch 9 completed in 3m 51s\n",
      "Loss 0.035160866142915824 is bigger than Loss 0.033584998630519385 in the prev epoch \n",
      "Loss = 0.03516087 at epoch 10 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.03277614 at epoch 11 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.03162937 at epoch 12 completed in 3m 51s\n",
      "Loss 0.034426711402567346 is bigger than Loss 0.03162936993342425 in the prev epoch \n",
      "Loss = 0.03442671 at epoch 13 completed in 3m 51s\n",
      "Loss 0.032192576139100954 is bigger than Loss 0.03162936993342425 in the prev epoch \n",
      "Loss = 0.03219258 at epoch 14 completed in 3m 51s\n",
      "Loss 0.03242374587271899 is bigger than Loss 0.03162936993342425 in the prev epoch \n",
      "Loss = 0.03242375 at epoch 15 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.02970230 at epoch 16 completed in 3m 51s\n",
      "Loss 0.03212519133463498 is bigger than Loss 0.029702301759804987 in the prev epoch \n",
      "Loss = 0.03212519 at epoch 17 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.02936679 at epoch 18 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.02721013 at epoch 19 completed in 3m 51s\n",
      "Loss 0.028364217016579856 is bigger than Loss 0.027210125836676775 in the prev epoch \n",
      "Loss = 0.02836422 at epoch 20 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.02705998 at epoch 21 completed in 3m 51s\n",
      "Loss 0.028886620599244316 is bigger than Loss 0.027059980066759255 in the prev epoch \n",
      "Loss = 0.02888662 at epoch 22 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.02675204 at epoch 23 completed in 3m 51s\n",
      "Loss 0.026861226155555674 is bigger than Loss 0.026752037280904396 in the prev epoch \n",
      "Loss = 0.02686123 at epoch 24 completed in 3m 51s\n",
      "Loss 0.027140100495889784 is bigger than Loss 0.026752037280904396 in the prev epoch \n",
      "Loss = 0.02714010 at epoch 25 completed in 3m 51s\n",
      "Loss 0.026836258356592476 is bigger than Loss 0.026752037280904396 in the prev epoch \n",
      "Loss = 0.02683626 at epoch 26 completed in 3m 51s\n",
      "Loss 0.027172746672960265 is bigger than Loss 0.026752037280904396 in the prev epoch \n",
      "Loss = 0.02717275 at epoch 27 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.02667031 at epoch 28 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.02454198 at epoch 29 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.02450372 at epoch 30 completed in 3m 51s\n",
      "Loss 0.02617320927126068 is bigger than Loss 0.024503721931417085 in the prev epoch \n",
      "Loss = 0.02617321 at epoch 31 completed in 3m 51s\n",
      "Loss 0.025043187702872914 is bigger than Loss 0.024503721931417085 in the prev epoch \n",
      "Loss = 0.02504319 at epoch 32 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.02233895 at epoch 33 completed in 3m 51s\n",
      "Loss 0.025275995765945726 is bigger than Loss 0.022338949536372527 in the prev epoch \n",
      "Loss = 0.02527600 at epoch 34 completed in 3m 51s\n",
      "Loss 0.02291809942706357 is bigger than Loss 0.022338949536372527 in the prev epoch \n",
      "Loss = 0.02291810 at epoch 35 completed in 3m 51s\n",
      "Loss 0.02319994198956659 is bigger than Loss 0.022338949536372527 in the prev epoch \n",
      "Loss = 0.02319994 at epoch 36 completed in 3m 51s\n",
      "Loss 0.024614862291408426 is bigger than Loss 0.022338949536372527 in the prev epoch \n",
      "Loss = 0.02461486 at epoch 37 completed in 3m 51s\n",
      "Learning rate changed from 0.002 to 0.0004\n",
      "Loss 0.022538464464513328 is bigger than Loss 0.022338949536372527 in the prev epoch \n",
      "Loss = 0.02253846 at epoch 38 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.01180582 at epoch 39 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.01116220 at epoch 40 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.01109449 at epoch 41 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.01090070 at epoch 42 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.01087378 at epoch 43 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.01037105 at epoch 44 completed in 3m 51s\n",
      "Loss 0.010903983023017642 is bigger than Loss 0.010371053655232694 in the prev epoch \n",
      "Loss = 0.01090398 at epoch 45 completed in 3m 51s\n",
      "Loss 0.010739725791583112 is bigger than Loss 0.010371053655232694 in the prev epoch \n",
      "Loss = 0.01073973 at epoch 46 completed in 3m 51s\n",
      "Loss 0.010881609244804299 is bigger than Loss 0.010371053655232694 in the prev epoch \n",
      "Loss = 0.01088161 at epoch 47 completed in 3m 51s\n",
      "Loss 0.010464565243039814 is bigger than Loss 0.010371053655232694 in the prev epoch \n",
      "Loss = 0.01046457 at epoch 48 completed in 3m 51s\n",
      "Learning rate changed from 0.0004 to 8e-05\n",
      "Loss 0.01045648952480404 is bigger than Loss 0.010371053655232694 in the prev epoch \n",
      "Loss = 0.01045649 at epoch 49 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00806713 at epoch 50 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00796347 at epoch 51 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00793937 at epoch 52 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00789614 at epoch 53 completed in 3m 51s\n",
      "Loss 0.008045766967614847 is bigger than Loss 0.007896144564396568 in the prev epoch \n",
      "Loss = 0.00804577 at epoch 54 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00784960 at epoch 55 completed in 3m 51s\n",
      "Loss 0.007892547417432044 is bigger than Loss 0.007849597376771272 in the prev epoch \n",
      "Loss = 0.00789255 at epoch 56 completed in 3m 51s\n",
      "Loss 0.007902351400948 is bigger than Loss 0.007849597376771272 in the prev epoch \n",
      "Loss = 0.00790235 at epoch 57 completed in 3m 51s\n",
      "Loss 0.007890459601767362 is bigger than Loss 0.007849597376771272 in the prev epoch \n",
      "Loss = 0.00789046 at epoch 58 completed in 3m 51s\n",
      "Loss 0.007882149053205337 is bigger than Loss 0.007849597376771272 in the prev epoch \n",
      "Loss = 0.00788215 at epoch 59 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00774790 at epoch 60 completed in 3m 51s\n",
      "Loss 0.007838768799390102 is bigger than Loss 0.007747897676724408 in the prev epoch \n",
      "Loss = 0.00783877 at epoch 61 completed in 3m 51s\n",
      "Loss 0.007925537801347684 is bigger than Loss 0.007747897676724408 in the prev epoch \n",
      "Loss = 0.00792554 at epoch 62 completed in 3m 51s\n",
      "Loss 0.007804382289094587 is bigger than Loss 0.007747897676724408 in the prev epoch \n",
      "Loss = 0.00780438 at epoch 63 completed in 3m 51s\n",
      "Loss 0.007838389658635226 is bigger than Loss 0.007747897676724408 in the prev epoch \n",
      "Loss = 0.00783839 at epoch 64 completed in 3m 51s\n",
      "Learning rate changed from 8e-05 to 1.6000000000000003e-05\n",
      "Loss 0.007781581228731994 is bigger than Loss 0.007747897676724408 in the prev epoch \n",
      "Loss = 0.00778158 at epoch 65 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00727898 at epoch 66 completed in 3m 51s\n",
      "Loss 0.007303334360143968 is bigger than Loss 0.007278984611454819 in the prev epoch \n",
      "Loss = 0.00730333 at epoch 67 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00720495 at epoch 68 completed in 3m 51s\n",
      "Loss 0.007302998313680292 is bigger than Loss 0.00720495027223868 in the prev epoch \n",
      "Loss = 0.00730300 at epoch 69 completed in 3m 51s\n",
      "Loss 0.007242341475295169 is bigger than Loss 0.00720495027223868 in the prev epoch \n",
      "Loss = 0.00724234 at epoch 70 completed in 3m 51s\n",
      "Loss 0.007218228806076306 is bigger than Loss 0.00720495027223868 in the prev epoch \n",
      "Loss = 0.00721823 at epoch 71 completed in 3m 51s\n",
      "Loss 0.007334562286601533 is bigger than Loss 0.00720495027223868 in the prev epoch \n",
      "Loss = 0.00733456 at epoch 72 completed in 3m 51s\n",
      "Learning rate changed from 1.6000000000000003e-05 to 3.2000000000000007e-06\n",
      "Loss 0.007265199084899255 is bigger than Loss 0.00720495027223868 in the prev epoch \n",
      "Loss = 0.00726520 at epoch 73 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00712301 at epoch 74 completed in 3m 51s\n",
      "Loss 0.007196163364819118 is bigger than Loss 0.007123005158001823 in the prev epoch \n",
      "Loss = 0.00719616 at epoch 75 completed in 3m 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.00710616 at epoch 76 completed in 3m 51s\n",
      "Loss 0.007131100287660955 is bigger than Loss 0.007106158824504486 in the prev epoch \n",
      "Loss = 0.00713110 at epoch 77 completed in 3m 51s\n",
      "Loss 0.007131896378871585 is bigger than Loss 0.007106158824504486 in the prev epoch \n",
      "Loss = 0.00713190 at epoch 78 completed in 3m 51s\n",
      "Loss 0.007129504175058433 is bigger than Loss 0.007106158824504486 in the prev epoch \n",
      "Loss = 0.00712950 at epoch 79 completed in 3m 51s\n",
      "Loss 0.0071670086695147395 is bigger than Loss 0.007106158824504486 in the prev epoch \n",
      "Loss = 0.00716701 at epoch 80 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00706064 at epoch 81 completed in 3m 51s\n",
      "Loss 0.007204137137824934 is bigger than Loss 0.007060635179680373 in the prev epoch \n",
      "Loss = 0.00720414 at epoch 82 completed in 3m 51s\n",
      "Loss 0.007092556115239858 is bigger than Loss 0.007060635179680373 in the prev epoch \n",
      "Loss = 0.00709256 at epoch 83 completed in 3m 51s\n",
      "Loss 0.007066373285571379 is bigger than Loss 0.007060635179680373 in the prev epoch \n",
      "Loss = 0.00706637 at epoch 84 completed in 3m 51s\n",
      "Loss 0.007097252936634633 is bigger than Loss 0.007060635179680373 in the prev epoch \n",
      "Loss = 0.00709725 at epoch 85 completed in 3m 51s\n",
      "Learning rate changed from 3.2000000000000007e-06 to 6.400000000000001e-07\n",
      "Loss 0.007092701629070299 is bigger than Loss 0.007060635179680373 in the prev epoch \n",
      "Loss = 0.00709270 at epoch 86 completed in 3m 51s\n",
      "Loss 0.007084214227007967 is bigger than Loss 0.007060635179680373 in the prev epoch \n",
      "Loss = 0.00708421 at epoch 87 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00703642 at epoch 88 completed in 3m 51s\n",
      "Loss 0.007084553287630635 is bigger than Loss 0.007036416479386389 in the prev epoch \n",
      "Loss = 0.00708455 at epoch 89 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00700234 at epoch 90 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00697406 at epoch 91 completed in 3m 51s\n",
      "Loss 0.00702408732314195 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00702409 at epoch 92 completed in 3m 51s\n",
      "Loss 0.007077492589929274 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00707749 at epoch 93 completed in 3m 51s\n",
      "Loss 0.007053790205557431 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00705379 at epoch 94 completed in 3m 51s\n",
      "Loss 0.007078782703195301 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00707878 at epoch 95 completed in 3m 51s\n",
      "Learning rate changed from 6.400000000000001e-07 to 1.2800000000000003e-07\n",
      "Loss 0.0070968748622440864 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00709687 at epoch 96 completed in 3m 51s\n",
      "Loss 0.0071420909983239 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00714209 at epoch 97 completed in 3m 51s\n",
      "Loss 0.007061913905532234 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00706191 at epoch 98 completed in 3m 51s\n",
      "Loss 0.007126632995371308 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00712663 at epoch 99 completed in 3m 51s\n",
      "Loss 0.007101278321019241 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00710128 at epoch 100 completed in 3m 51s\n",
      "Learning rate changed from 1.2800000000000003e-07 to 2.5600000000000008e-08\n",
      "Loss 0.007023820181909417 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00702382 at epoch 101 completed in 3m 51s\n",
      "Loss 0.007108687913444425 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00710869 at epoch 102 completed in 3m 51s\n",
      "Loss 0.007043881512113979 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00704388 at epoch 103 completed in 3m 51s\n",
      "Loss 0.00703578700378005 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00703579 at epoch 104 completed in 3m 51s\n",
      "Loss 0.007130125188933953 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00713013 at epoch 105 completed in 3m 51s\n",
      "Learning rate changed from 2.5600000000000008e-08 to 5.120000000000001e-09\n",
      "Loss 0.0070726601420236495 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00707266 at epoch 106 completed in 3m 51s\n",
      "Loss 0.00704492950146752 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00704493 at epoch 107 completed in 3m 51s\n",
      "Loss 0.007004538442540381 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00700454 at epoch 108 completed in 3m 51s\n",
      "Loss 0.006994612881514643 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00699461 at epoch 109 completed in 3m 51s\n",
      "Loss 0.007057054531760514 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00705705 at epoch 110 completed in 3m 51s\n",
      "Learning rate changed from 5.120000000000001e-09 to 1.0240000000000002e-09\n",
      "Loss 0.007028717606860611 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00702872 at epoch 111 completed in 3m 51s\n",
      "Loss 0.007093313610447304 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00709331 at epoch 112 completed in 3m 51s\n",
      "Loss 0.007061430012940296 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00706143 at epoch 113 completed in 3m 51s\n",
      "Loss 0.007058307579053297 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00705831 at epoch 114 completed in 3m 51s\n",
      "Loss 0.00702762142622045 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00702762 at epoch 115 completed in 3m 51s\n",
      "Learning rate changed from 1.0240000000000002e-09 to 2.0480000000000003e-10\n",
      "Loss 0.007134582667744586 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00713458 at epoch 116 completed in 3m 51s\n",
      "Loss 0.00706660560997469 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00706661 at epoch 117 completed in 3m 51s\n",
      "Loss 0.00705815182133977 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00705815 at epoch 118 completed in 3m 51s\n",
      "Loss 0.007120903339902205 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00712090 at epoch 119 completed in 3m 51s\n",
      "Loss 0.007064405727599348 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00706441 at epoch 120 completed in 3m 51s\n",
      "Learning rate changed from 2.0480000000000003e-10 to 4.0960000000000006e-11\n",
      "Loss 0.007020487311695303 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00702049 at epoch 121 completed in 3m 51s\n",
      "Loss 0.007156043226298476 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00715604 at epoch 122 completed in 3m 51s\n",
      "Loss 0.007077577433415821 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00707758 at epoch 123 completed in 3m 51s\n",
      "Loss 0.007026748084463179 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00702675 at epoch 124 completed in 3m 51s\n",
      "Loss 0.007117513595148921 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00711751 at epoch 125 completed in 3m 51s\n",
      "Learning rate changed from 4.0960000000000006e-11 to 8.192e-12\n",
      "Loss 0.007124400270570602 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00712440 at epoch 126 completed in 3m 51s\n",
      "Loss 0.0070567327864202 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00705673 at epoch 127 completed in 3m 51s\n",
      "Loss 0.007135198597264077 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00713520 at epoch 128 completed in 3m 51s\n",
      "Loss 0.007133623615040311 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00713362 at epoch 129 completed in 3m 51s\n",
      "Loss 0.0070880623733890905 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00708806 at epoch 130 completed in 3m 51s\n",
      "Learning rate changed from 8.192e-12 to 1.6384000000000002e-12\n",
      "Loss 0.0070856547342347245 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00708565 at epoch 131 completed in 3m 51s\n",
      "Loss 0.007036055565279509 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00703606 at epoch 132 completed in 3m 51s\n",
      "Loss 0.007108489237060504 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00710849 at epoch 133 completed in 3m 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.007139070884191565 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00713907 at epoch 134 completed in 3m 51s\n",
      "Loss 0.007016931077731505 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00701693 at epoch 135 completed in 3m 51s\n",
      "Learning rate changed from 1.6384000000000002e-12 to 3.2768e-13\n",
      "Loss 0.007077275696210563 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00707728 at epoch 136 completed in 3m 51s\n",
      "Loss 0.007071760159784129 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00707176 at epoch 137 completed in 3m 51s\n",
      "Loss 0.007010557544417679 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00701056 at epoch 138 completed in 3m 51s\n",
      "Loss 0.007093443471406187 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00709344 at epoch 139 completed in 3m 51s\n",
      "Loss 0.007091804016381502 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00709180 at epoch 140 completed in 3m 51s\n",
      "Learning rate changed from 3.2768e-13 to 6.5536e-14\n",
      "Loss 0.0070874299587948 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00708743 at epoch 141 completed in 3m 51s\n",
      "Loss 0.007130320402793587 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00713032 at epoch 142 completed in 3m 51s\n",
      "Loss 0.007061768259986172 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00706177 at epoch 143 completed in 3m 51s\n",
      "Loss 0.007128193112356324 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00712819 at epoch 144 completed in 3m 51s\n",
      "Loss 0.007036433684905723 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00703643 at epoch 145 completed in 3m 51s\n",
      "Learning rate changed from 6.5536e-14 to 1.3107200000000001e-14\n",
      "Loss 0.007110297252157968 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00711030 at epoch 146 completed in 3m 51s\n",
      "Loss 0.007022869445250502 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00702287 at epoch 147 completed in 3m 51s\n",
      "Loss 0.007107317146978208 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00710732 at epoch 148 completed in 3m 51s\n",
      "Loss 0.007068890721670218 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00706889 at epoch 149 completed in 3m 51s\n",
      "Loss 0.007094843860582582 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00709484 at epoch 150 completed in 3m 51s\n",
      "Learning rate changed from 1.3107200000000001e-14 to 2.6214400000000003e-15\n",
      "Loss 0.007128090928973896 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00712809 at epoch 151 completed in 3m 51s\n",
      "Loss 0.007084766774039184 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00708477 at epoch 152 completed in 3m 51s\n",
      "Loss 0.007027224659520601 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00702722 at epoch 153 completed in 3m 51s\n",
      "Loss 0.006977781230982924 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00697778 at epoch 154 completed in 3m 51s\n",
      "Loss 0.00706477951258421 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00706478 at epoch 155 completed in 3m 51s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 2.6214400000000003e-15 to 1e-15\n",
      "Loss 0.007105632498195129 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00710563 at epoch 156 completed in 3m 51s\n",
      "Loss 0.0070000076387077585 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00700001 at epoch 157 completed in 3m 51s\n",
      "Loss 0.007067946451716125 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00706795 at epoch 158 completed in 3m 51s\n",
      "Loss 0.007175416556586113 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00717542 at epoch 159 completed in 3m 51s\n",
      "Loss 0.00707579057131495 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00707579 at epoch 160 completed in 3m 51s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0070270172952275195 is bigger than Loss 0.006974061537933134 in the prev epoch \n",
      "Loss = 0.00702702 at epoch 161 completed in 3m 51s\n",
      "save the weights\n",
      "Loss = 0.00696893 at epoch 162 completed in 3m 51s\n",
      "Loss 0.007042373255826533 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00704237 at epoch 163 completed in 3m 51s\n",
      "Loss 0.007116906576390777 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00711691 at epoch 164 completed in 3m 51s\n",
      "Loss 0.007032002420829875 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00703200 at epoch 165 completed in 3m 51s\n",
      "Loss 0.007083229963401599 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00708323 at epoch 166 completed in 3m 51s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.007101886213224913 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00710189 at epoch 167 completed in 3m 51s\n",
      "Loss 0.007013152773225946 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00701315 at epoch 168 completed in 3m 51s\n",
      "Loss 0.007055385269756827 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00705539 at epoch 169 completed in 3m 51s\n",
      "Loss 0.007016311897896229 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00701631 at epoch 170 completed in 3m 51s\n",
      "Loss 0.007020606028049121 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00702061 at epoch 171 completed in 3m 51s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.007033665013233466 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00703367 at epoch 172 completed in 3m 51s\n",
      "Loss 0.007087050563256656 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00708705 at epoch 173 completed in 3m 51s\n",
      "Loss 0.00714104721921363 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00714105 at epoch 174 completed in 3m 51s\n",
      "Loss 0.00706077885680965 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00706078 at epoch 175 completed in 3m 51s\n",
      "Loss 0.006992501859287063 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00699250 at epoch 176 completed in 3m 51s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.007006401458888183 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00700640 at epoch 177 completed in 3m 51s\n",
      "Loss 0.007044494170030313 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00704449 at epoch 178 completed in 3m 51s\n",
      "Loss 0.007082373267039656 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00708237 at epoch 179 completed in 3m 51s\n",
      "Loss 0.0070814357065994826 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00708144 at epoch 180 completed in 3m 51s\n",
      "Loss 0.007045813586030686 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00704581 at epoch 181 completed in 3m 51s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.00706279176087784 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00706279 at epoch 182 completed in 3m 51s\n",
      "Loss 0.007064584258145521 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00706458 at epoch 183 completed in 3m 51s\n",
      "Loss 0.007056496636276798 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00705650 at epoch 184 completed in 3m 51s\n",
      "Loss 0.006978325642246221 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00697833 at epoch 185 completed in 3m 51s\n",
      "Loss 0.007060567094013097 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00706057 at epoch 186 completed in 3m 51s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.007032653550351303 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00703265 at epoch 187 completed in 3m 51s\n",
      "Loss 0.007041417023033966 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00704142 at epoch 188 completed in 3m 51s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.007031746961043349 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00703175 at epoch 189 completed in 3m 51s\n",
      "Loss 0.007057419902911144 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00705742 at epoch 190 completed in 3m 51s\n",
      "Loss 0.007021144116962595 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00702114 at epoch 191 completed in 3m 51s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.0070888581858681785 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00708886 at epoch 192 completed in 3m 51s\n",
      "Loss 0.00716042183672211 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00716042 at epoch 193 completed in 3m 51s\n",
      "Loss 0.007024540818695511 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00702454 at epoch 194 completed in 3m 51s\n",
      "Loss 0.007088051912640887 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00708805 at epoch 195 completed in 3m 51s\n",
      "Loss 0.0070638381576697724 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00706384 at epoch 196 completed in 3m 51s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.007023583147674799 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00702358 at epoch 197 completed in 3m 51s\n",
      "Loss 0.007061080622619816 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00706108 at epoch 198 completed in 3m 51s\n",
      "Loss 0.007184648871022676 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00718465 at epoch 199 completed in 3m 51s\n",
      "Loss 0.007061790164027896 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00706179 at epoch 200 completed in 3m 51s\n",
      "Loss 0.007060418640529472 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00706042 at epoch 201 completed in 3m 51s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.007100153755662697 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00710015 at epoch 202 completed in 3m 51s\n",
      "Loss 0.006982716147654823 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00698272 at epoch 203 completed in 3m 51s\n",
      "Loss 0.0070472988007324105 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00704730 at epoch 204 completed in 3m 51s\n",
      "Loss 0.007037247430666217 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00703725 at epoch 205 completed in 3m 51s\n",
      "Loss 0.007062549895739981 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00706255 at epoch 206 completed in 3m 51s\n",
      "Cant decrease further!!\n",
      "Learning rate changed from 1e-15 to 1e-15\n",
      "Loss 0.007058955895980554 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00705896 at epoch 207 completed in 3m 51s\n",
      "Loss 0.006988902130563344 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00698890 at epoch 208 completed in 3m 51s\n",
      "Loss 0.007057086079647498 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00705709 at epoch 209 completed in 3m 51s\n",
      "Loss 0.007080971728345111 is bigger than Loss 0.006968932731875354 in the prev epoch \n",
      "Loss = 0.00708097 at epoch 210 completed in 3m 51s\n"
     ]
    }
   ],
   "source": [
    "#MUST UNCOMMENT BELOW LINE...\n",
    "    \n",
    "net = net.cuda()\n",
    "\n",
    "#loading the model after the weights of epoch50.. to check what loss the model gives if lr is taken as 0.0001\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.002, momentum=0.9)\n",
    "\n",
    "#criterion = RMSELoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = EuclideanLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = CosineLoss()\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "criterion = CustomLoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = CosineLoss()\n",
    "criterion.cuda()\n",
    "\n",
    "currepochloss =0.03734306#float('Inf')\n",
    "#epochs, n_examples, i, batch_size, flag = 1,5900, 0, 5, 0\n",
    "epochs, n_examples, i, batch_size, flag = 210, 35000, 0, 50, 0\n",
    "\n",
    "\n",
    "while i != epochs:\n",
    "    since = time.time()\n",
    "    cost, batchloss = 0.0, 0.0\n",
    "    num_batches = n_examples//batch_size\n",
    "    #print num_batches    #indices = np.random.permutation(5600)\n",
    "    #indices = np.random.permutation(3524)\n",
    "    \n",
    "    #indices = np.random.permutation(5900)\n",
    "    indices = np.random.permutation(n_examples)\n",
    "    samplesUnprocessed = np.size(indices)\n",
    "    \n",
    "    #batchwise training starts here...\n",
    "    for k in range(num_batches):\n",
    "        since1 = time.time()\n",
    "       # print(\"bacth number:\"+str(k))\n",
    "        xtrain3 = torch.FloatTensor(batch_size,3,180,320)\n",
    "        ytrain3 = torch.FloatTensor(batch_size,1)\n",
    "        ##validPixel = torch.FloatTensor(batch_size,480,640)\n",
    "        \n",
    "        for ind in range(batch_size):\n",
    "            #ind1 = np.random.randint(0,5599)\n",
    "            ind1 = np.random.randint(0,samplesUnprocessed)\n",
    "            #ind1 = np.random.randint(0,794)\n",
    "            #ind1 = np.random.randint(0,794)            \n",
    "            newxind = indices[ind1]            \n",
    "            xtrain3[ind] = xtrainT[newxind]\n",
    "            ytrain3[ind] = ytrainT[newxind,0,0]\n",
    "            ##validPixel[ind] = imgValidTrain2[newxind]\n",
    "            \n",
    "            #print ytrain3[ind,0,0,0], ytrain2[newxind,0,0,0]\n",
    "            indices = np.delete(indices,ind1)\n",
    "            samplesUnprocessed = samplesUnprocessed - 1\n",
    "        \n",
    "        #start, end = k*batch_size, (k+1)*batch_size\n",
    "        #batchloss = train(model5,criterion, optimizer, xtrain3, ytrain3, validPixel,batch_size)\n",
    "        batchloss = train(net,criterion, optimizer, xtrain3, ytrain3, batch_size)\n",
    "        batch_time = time.time() - since1\n",
    "        #cost += batchloss\n",
    "        cost = (cost*k+batchloss)/(k+1)\n",
    "        #print k,cost\n",
    "        #print(\"No. of samples UnProcessed \"+str(samplesUnprocessed))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    epochloss = cost #/num_batches\n",
    "    \n",
    "    if epochloss < currepochloss:\n",
    "        print('save the weights')\n",
    "        torch.save(net.state_dict(),\"./weights/CustomLoss_new/CustomLoss_new_35000_INCEP_NET_new_ANGLE_240_2_epochs.pth\")\n",
    "        flag = 0\n",
    "        currepochloss = epochloss\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "        if flag == 5:\n",
    "            for p in optimizer.param_groups:\n",
    "                lr2 = p['lr']\n",
    "            newlr = lr2/5\n",
    "            \n",
    "            if newlr < 1e-15:\n",
    "                print(\"Cant decrease further!!\")\n",
    "                newlr = 1e-15\n",
    "            flag = 0 \n",
    "            optimizer = optim.SGD(net.parameters(), lr=newlr, momentum=0.9)\n",
    "            print(\"Learning rate changed from \"+str(lr2)+\" to \"+str(newlr))\n",
    "            \n",
    "        print(\"Loss \"+str(epochloss)+\" is bigger than Loss \"+str(currepochloss)+\" in the prev epoch \")\n",
    "        \n",
    "    print('Loss = {:.8f} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(epochloss,(i+1),(time_elapsed//60),(time_elapsed%60)))\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-15\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for params in optimizer.param_groups:\n",
    "    print(params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()\n",
    "net.load_state_dict(torch.load(\"./weights/CustomLoss_new/ANGLE/CustomLoss_new_35000_107_epochs.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalpred size is --->  torch.Size([600, 1])\n",
      "num of batches ---> 30\n"
     ]
    }
   ],
   "source": [
    "#testing of the architecture...\n",
    "num_batches = 0\n",
    "#6 evenly divides the test batch size..\n",
    "test_batch_size = 20\n",
    "n_examples = 600\n",
    "#finalpred = Variable(torch.zeros((n_examples,3,120,160)))\n",
    "finalpred1 = Variable(torch.zeros((n_examples,1)))\n",
    "finalpred2 = Variable(torch.zeros((n_examples,1)))\n",
    "print(\"finalpred size is ---> \", finalpred.size())\n",
    "\n",
    "num_batches = n_examples//test_batch_size\n",
    "print(\"num of batches --->\", num_batches)\n",
    "for k in range(num_batches):\n",
    "    start, end = k*test_batch_size, (k+1)*test_batch_size\n",
    "    output1,output2 = net.forward(Variable(xtestT[start:end], volatile=True).cuda())\n",
    "    finalpred1[start:end] = output1\n",
    "    finalpred2[start:end] = output2\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(600, 1)\n",
      "(600, 1)\n"
     ]
    }
   ],
   "source": [
    "data1 = finalpred1.data.numpy()\n",
    "data2 = finalpred2.data.numpy()\n",
    "print(data1.shape)\n",
    "print(data2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600])\n",
      "MSElossRad==0.0017291542736233901 ABSlossRad==0.02546382963657379 RELlossRad0.021039048300262948\n",
      "MSElossDeg==0.09907324200562978 ABSlossDeg==1.458969968415823 RELlossDeg1.2054486725769553\n"
     ]
    }
   ],
   "source": [
    "dif = torch.abs(finalpred1.data[:,0]-ytestT[:,0,0])\n",
    "dif1 = torch.abs((finalpred1.data[:,0]-ytestT[:,0,0])/ytestT[:,0,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "\n",
    "MSElossRad = torch.mean(torch.pow(dif,2))\n",
    "ABSlossRad = torch.mean(dif)\n",
    "RELlossRad = torch.mean(dif1)\n",
    "MSElossDeg = MSElossRad*(180/np.pi)\n",
    "ABSlossDeg = ABSlossRad*(180/np.pi)\n",
    "RELlossDeg = RELlossRad*(180/np.pi)\n",
    "print(\"MSElossRad==\"+str(MSElossRad),\"ABSlossRad==\"+str(ABSlossRad),\"RELlossRad\"+str(RELlossRad))\n",
    "print(\"MSElossDeg==\"+str(MSElossDeg),\"ABSlossDeg==\"+str(ABSlossDeg),\"RELlossDeg\"+str(RELlossDeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600])\n",
      "MSElossRad==0.002082098455178928 ABSlossRad==0.025291017591953277 RELlossRad0.021111476879756688\n",
      "MSElossDeg==0.11929545401246118 ABSlossDeg==1.4490685676100412 RELlossDeg1.2095985244980745\n"
     ]
    }
   ],
   "source": [
    "dif = torch.abs(finalpred2.data[:,0]-ytestT[:,0,0])\n",
    "dif1 = torch.abs((finalpred2.data[:,0]-ytestT[:,0,0])/ytestT[:,0,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "\n",
    "MSElossRad = torch.mean(torch.pow(dif,2))\n",
    "ABSlossRad = torch.mean(dif)\n",
    "RELlossRad = torch.mean(dif1)\n",
    "MSElossDeg = MSElossRad*(180/np.pi)\n",
    "ABSlossDeg = ABSlossRad*(180/np.pi)\n",
    "RELlossDeg = RELlossRad*(180/np.pi)\n",
    "print(\"MSElossRad==\"+str(MSElossRad),\"ABSlossRad==\"+str(ABSlossRad),\"RELlossRad\"+str(RELlossRad))\n",
    "print(\"MSElossDeg==\"+str(MSElossDeg),\"ABSlossDeg==\"+str(ABSlossDeg),\"RELlossDeg\"+str(RELlossDeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
