{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/cvrlab/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import re\n",
    "import hickle as hkl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.legacy.nn import Reshape\n",
    "import graphviz\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "#from visualize import make_dot\n",
    "import torch.optim as optim\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "import torch.utils.data as utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "import time\n",
    "import logging\n",
    "from math import log,sqrt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.current_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net_101 = models.resnet101(pretrained=True)\n",
    "#dense161"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#res_net_101= nn.Sequential(*list(res_net_101.children())[:-2])\n",
    "res_net_101\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input=Variable(torch.randn(1,3,180,320))\n",
    "# output=res_net_101(input)\n",
    "# print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitializeWeights(mod):\n",
    "    for m in mod.modules():\n",
    "        if isinstance(m,nn.Conv2d):\n",
    "            n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "            #print m.weight.size(), m.out_channels, m.in_channels\n",
    "            m.weight.data.normal_(0,sqrt(2./n))\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            m.weight.data.fill_(1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Linear):\n",
    "            m.bias.data.zero_()    \n",
    "    return mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Sequential(nn.BatchNorm2d(2048),nn.ReLU(),nn.Conv2d(2048,1024,1))\n",
    "conv1 = InitializeWeights(conv1)\n",
    "conv2 = nn.Sequential(nn.BatchNorm2d(1024),nn.ReLU(),nn.Conv2d(1024,128,5))\n",
    "conv2 = InitializeWeights(conv2)\n",
    "conv3 = nn.Sequential(nn.BatchNorm2d(128),nn.ReLU(),nn.Conv2d(128,8,1))\n",
    "conv3 = InitializeWeights(conv3)\n",
    "norm1 = nn.BatchNorm2d(8)\n",
    "norm1 = InitializeWeights(norm1)\n",
    "fc1 = nn.Linear(96, 1)\n",
    "fc1 = InitializeWeights(fc1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel4(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(MyModel4, self).__init__()\n",
    "        self.pretrained_model = nn.Sequential(*list(res_net_101.children())[:-2])\n",
    "        self.conv1 = conv1\n",
    "        self.conv2 = conv2\n",
    "        self.conv3 = conv3\n",
    "        self.norm1 = norm1\n",
    "        self.fc1 = fc1\n",
    "   \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pretrained_model(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.norm1(x)\n",
    "        #print(x.size())\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        #print(x.size())\n",
    "        #x = self.conv4(x)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "#print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = MyModel4(res_net_101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(net)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4689431190490723\n",
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "input=Variable(torch.randn(5,3,180,320))\n",
    "tic=time.time()\n",
    "output=net(input)\n",
    "tac=time.time()\n",
    "print(tac-tic)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sum1 = 0\n",
    "        \n",
    "print(\"Number of layers ---> \",len(list(net.parameters())))\n",
    "for params in res_net_152.parameters():\n",
    "    if params.requires_grad == True:\n",
    "        sum1 += params.numel()\n",
    "    \n",
    "print(\"Total number of parameters ---> \",sum1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(5, 3, 180, 320))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "239.47545647621155\n"
     ]
    }
   ],
   "source": [
    "tic_1=time.time()\n",
    "file = h5py.File('./DATASET/CODE/NewTrainData_59_cor__35000.h5')\n",
    "xtrainT = torch.from_numpy(np.array(file['xtrain'],dtype=np.float32)).float()\n",
    "ytrainT = torch.from_numpy(np.array(file['ytrain'],dtype=np.float32)).float()\n",
    "#xtrain = np.array(file['xtrain'],dtype=np.float32)\n",
    "#ytrain = np.array(file['ytrain'],dtype=np.float32)\n",
    "toc_1=time.time()\n",
    "print(toc_1-tic_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = h5py.File('./DATASET/CODE/NewTestData_22_cor_random_2.h5')\n",
    "xtestT = torch.from_numpy(np.array(file['xtest'],dtype=np.float32)).float()\n",
    "ytestT = torch.from_numpy(np.array(file['ytest'],dtype=np.float32)).float()\n",
    "#xtest = np.array(file['xtest'],dtype=np.float32)\n",
    "#ytest = np.array(file['ytest'],dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_rgb_to_bgr(batch):\n",
    "    #print(batch.size())\n",
    "    (r, g, b) = torch.chunk(batch, 3, 1)\n",
    "    #print(r.size())\n",
    "    batch1 = torch.cat((b, g, r),1)\n",
    "    #print(batch1.size())\n",
    "    return batch1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = batch_rgb_to_bgr(xtrainT)\n",
    "xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#print(xtrainT.size(), xtestT.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrainT = torch.div(xtrainT,255.0)\n",
    "xtestT = torch.div(xtestT,255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xtrainT.size(), ytrainT.size(), xtestT.size(), ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalize(object):\n",
    "    \"\"\"\n",
    "    Normalize an tensor image with mean and standard deviation.\n",
    "    Given mean: (R, G, B) and std: (R, G, B),\n",
    "    will normalize each channel of the torch.*Tensor, i.e.\n",
    "    channel = (channel - mean) / std\n",
    "    Args:\n",
    "        mean (sequence): Sequence of means for R, G, B channels respecitvely.\n",
    "        std (sequence): Sequence of standard deviations for R, G, B channels\n",
    "            respecitvely.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # TODO: make efficient\n",
    "        for t, m, s in zip(tensor, self.mean, self.std):\n",
    "            t.sub_(m).div_(s)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "#xtrainT = norm(xtrainT)\n",
    "xtestT = norm(xtestT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(xtrainT), torch.max(xtrainT), torch.min(xtestT), torch.max(xtestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtestT = batch_rgb_to_bgr(xtestT)\n",
    "#xtestT = torch.div(xtestT,255.0)\n",
    "#mn = [0.406,0.456,0.485]\n",
    "#sd = [0.225,0.224,0.229]\n",
    "#norm = Normalize(mn,sd)\n",
    "#xtestT = norm(xtestT)\n",
    "#print(xtestT.size(), ytestT.size())\n",
    "#print(torch.min(xtestT), torch.max(xtestT),torch.min(ytestT), torch.max(ytestT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##def train(model, loss, optimizer, x_val, y_val, validPixel, batch_sz):\n",
    "def train(model, loss, optimizer, x_val, y_val,batch_size):\n",
    "    x = Variable(x_val,requires_grad = False).cuda()\n",
    "    y = Variable(y_val,requires_grad = False).cuda()\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    x = batch_rgb_to_bgr(x)\n",
    "    x = torch.div(x,255.0)\n",
    "    mn = [0.406,0.456,0.485]\n",
    "    sd = [0.225,0.224,0.229]\n",
    "    x[:,0,:,:] = (x[:,0,:,:]-mn[0])/sd[0]\n",
    "    x[:,1,:,:] = (x[:,1,:,:]-mn[1])/sd[1]\n",
    "    x[:,2,:,:] = (x[:,2,:,:]-mn[2])/sd[2]\n",
    "    \n",
    "    fx = model.forward(x)\n",
    "    \n",
    "    #print fx.data[0][0][64][87]\n",
    "    #fx = model5.forward(Variable(xtest2[start:end], volatile=True).cuda())\n",
    "    ##output = loss.forward(fx,y,validPixel,batch_sz)\n",
    "    output = loss.forward(fx,y)\n",
    "    #output = loss(fx, y)\n",
    "    output.backward()\n",
    "    \n",
    "    optimizer.step()\n",
    "    \n",
    "    return output.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom loss function.... this will be reverse Huber...\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        #k = torch.mean(inp[:,0])\n",
    "        '''\n",
    "        if (k >= 1.48 and k <= 1.65):\n",
    "            diff = torch.abs(tar[:,1]-inp[:,1])\n",
    "            loss = torch.mean(torch.pow(diff,2))\n",
    "        else:\n",
    "        '''\n",
    "        diff = torch.abs(tar[:,0]-inp[:,0]) #*(180/np.pi)\n",
    "        loss = torch.mean(diff)\n",
    "        #print(loss)\n",
    "        return loss\n",
    "        '''\n",
    "        c1 = c.data[0] \n",
    "        temp = diff > c1\n",
    "        check1 = torch.prod(temp)\n",
    "        \n",
    "        if check1 == 0:\n",
    "            lossval = torch.mean(diff)\n",
    "        else:\n",
    "            temp4 = torch.pow(diff,2)\n",
    "            d = torch.pow(c,2)\n",
    "            temp4 = temp4.add(d.expand_as(temp4))\n",
    "            lossval = torch.mean(temp4/(2*c))\n",
    "        return lossval\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class BerhuLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BerhuLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar):\n",
    "        #target is the ground truth value...\n",
    "        mt = tar[:,0]\n",
    "        mp = inp[:,0]\n",
    "        diff = torch.abs(mt-mp)        \n",
    "        lossval = 0.0        \n",
    "        c = 0.2 * torch.max(diff)\n",
    "        l1 = torch.mean(diff)\n",
    "        l2 = torch.mean(torch.pow(diff,2))\n",
    "        if l1 <= c:\n",
    "            lossval = l1\n",
    "        else:\n",
    "            lossval = (l2+c**2)/(2*c)\n",
    "        \n",
    "        return lossval\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#alpha = torch.FloatTensor(ytrainT[5,0])\n",
    "alpha = ytrainT[5,0]\n",
    "#print(alpha.shape)\n",
    "xt = torch.FloatTensor([np.cos(alpha),np.sin(alpha)])\n",
    "print(ytrainT[5,0],xt.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = ytrainT[5:10,0]\n",
    "print(torch.cos(alpha[0:1]-alpha[1:2]))\n",
    "xt = torch.stack([torch.cos(alpha[0:1]),torch.sin(alpha[0:1])])\n",
    "xp = torch.stack([torch.cos(alpha[1:2]),torch.sin(alpha[1:2])])\n",
    "print(xt[0],xt[1])\n",
    "#print(los)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CosineLoss, self).__init__()\n",
    "        \n",
    "    def forward(self,inp, tar,batch_sz):\n",
    "        alpha_t = tar[:,0]\n",
    "        alpha_p = inp[:,0]\n",
    "        #xt = torch.stack([torch.cos(alpha_t),torch.sin(alpha_t)])\n",
    "        #xp = torch.stack([torch.cos(alpha_p),torch.sin(alpha_p)])\n",
    "        #cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        #loss = cos(xt, xp)\n",
    "        #return loss\n",
    "        loss = Variable(torch.FloatTensor(batch_sz).zero_(), requires_grad=False).cuda()\n",
    "        for i in range(batch_sz):          \n",
    "            loss[i] = torch.cos(alpha_t[i:i+1]-alpha_p[i:i+1])\n",
    "            \n",
    "        lossval = 1.0-torch.mean(loss)    \n",
    "        #print(lossval)\n",
    "        return lossval\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.03395934 at epoch 1 completed in 6m 53s\n",
      "save the weights\n",
      "Loss = 0.02855008 at epoch 2 completed in 6m 54s\n",
      "save the weights\n",
      "Loss = 0.02565218 at epoch 3 completed in 6m 54s\n",
      "save the weights\n",
      "Loss = 0.02370458 at epoch 4 completed in 6m 53s\n",
      "save the weights\n",
      "Loss = 0.02154289 at epoch 5 completed in 6m 54s\n",
      "save the weights\n",
      "Loss = 0.01852184 at epoch 6 completed in 6m 54s\n",
      "save the weights\n",
      "Loss = 0.01696530 at epoch 7 completed in 6m 54s\n",
      "save the weights\n",
      "Loss = 0.01636159 at epoch 8 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.01561670 at epoch 9 completed in 6m 54s\n",
      "save the weights\n",
      "Loss = 0.01438827 at epoch 10 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.01360718 at epoch 11 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.01271634 at epoch 12 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.01199157 at epoch 13 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.01182943 at epoch 14 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.01155544 at epoch 15 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.01098550 at epoch 16 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.01051935 at epoch 17 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.01044451 at epoch 18 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.01032827 at epoch 19 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.01010401 at epoch 20 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00928983 at epoch 21 completed in 6m 55s\n",
      "Loss 0.009647980280752689 is bigger than Loss 0.009289825207420742 in the prev epoch \n",
      "Loss = 0.00964798 at epoch 22 completed in 6m 55s\n",
      "Loss 0.00967316692322492 is bigger than Loss 0.009289825207420742 in the prev epoch \n",
      "Loss = 0.00967317 at epoch 23 completed in 6m 55s\n",
      "Loss 0.009500608397115537 is bigger than Loss 0.009289825207420742 in the prev epoch \n",
      "Loss = 0.00950061 at epoch 24 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00907652 at epoch 25 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00871878 at epoch 26 completed in 6m 55s\n",
      "Loss 0.009036179945937244 is bigger than Loss 0.008718782868768472 in the prev epoch \n",
      "Loss = 0.00903618 at epoch 27 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00818165 at epoch 28 completed in 6m 55s\n",
      "Loss 0.008271452006750859 is bigger than Loss 0.008181650923831135 in the prev epoch \n",
      "Loss = 0.00827145 at epoch 29 completed in 6m 55s\n",
      "Loss 0.008521524485998912 is bigger than Loss 0.008181650923831135 in the prev epoch \n",
      "Loss = 0.00852152 at epoch 30 completed in 6m 55s\n",
      "Loss 0.008757192523617832 is bigger than Loss 0.008181650923831135 in the prev epoch \n",
      "Loss = 0.00875719 at epoch 31 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00792257 at epoch 32 completed in 6m 55s\n",
      "Loss 0.008221765931163517 is bigger than Loss 0.00792257141109023 in the prev epoch \n",
      "Loss = 0.00822177 at epoch 33 completed in 6m 55s\n",
      "Loss 0.008137615489906491 is bigger than Loss 0.00792257141109023 in the prev epoch \n",
      "Loss = 0.00813762 at epoch 34 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00791593 at epoch 35 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00783665 at epoch 36 completed in 6m 55s\n",
      "Loss 0.007882065546299732 is bigger than Loss 0.007836651760818712 in the prev epoch \n",
      "Loss = 0.00788207 at epoch 37 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00709813 at epoch 38 completed in 6m 55s\n",
      "Loss 0.007495069001402173 is bigger than Loss 0.007098128677478858 in the prev epoch \n",
      "Loss = 0.00749507 at epoch 39 completed in 6m 55s\n",
      "Loss 0.0076173989791423055 is bigger than Loss 0.007098128677478858 in the prev epoch \n",
      "Loss = 0.00761740 at epoch 40 completed in 6m 55s\n",
      "Loss 0.007832685040842214 is bigger than Loss 0.007098128677478858 in the prev epoch \n",
      "Loss = 0.00783269 at epoch 41 completed in 6m 55s\n",
      "Loss 0.007269887307392699 is bigger than Loss 0.007098128677478858 in the prev epoch \n",
      "Loss = 0.00726989 at epoch 42 completed in 6m 55s\n",
      "Learning rate changed from 0.002 to 0.0004\n",
      "Loss 0.00720353616108852 is bigger than Loss 0.007098128677478858 in the prev epoch \n",
      "Loss = 0.00720354 at epoch 43 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00473432 at epoch 44 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00450010 at epoch 45 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00447687 at epoch 46 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00428326 at epoch 47 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00423638 at epoch 48 completed in 6m 55s\n",
      "Loss 0.004238400440397005 is bigger than Loss 0.004236377161528381 in the prev epoch \n",
      "Loss = 0.00423840 at epoch 49 completed in 6m 55s\n",
      "Loss 0.004264682679304054 is bigger than Loss 0.004236377161528381 in the prev epoch \n",
      "Loss = 0.00426468 at epoch 50 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00406401 at epoch 51 completed in 6m 55s\n",
      "Loss 0.004096337708112383 is bigger than Loss 0.004064006558220294 in the prev epoch \n",
      "Loss = 0.00409634 at epoch 52 completed in 6m 55s\n",
      "Loss 0.004098217363070158 is bigger than Loss 0.004064006558220294 in the prev epoch \n",
      "Loss = 0.00409822 at epoch 53 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00400935 at epoch 54 completed in 6m 55s\n",
      "Loss 0.004101259508968466 is bigger than Loss 0.004009347641840578 in the prev epoch \n",
      "Loss = 0.00410126 at epoch 55 completed in 6m 55s\n",
      "Loss 0.00403288992015379 is bigger than Loss 0.004009347641840578 in the prev epoch \n",
      "Loss = 0.00403289 at epoch 56 completed in 6m 55s\n",
      "Loss 0.004139299647084307 is bigger than Loss 0.004009347641840578 in the prev epoch \n",
      "Loss = 0.00413930 at epoch 57 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00391674 at epoch 58 completed in 6m 55s\n",
      "Loss 0.0040705295849059325 is bigger than Loss 0.0039167440932776285 in the prev epoch \n",
      "Loss = 0.00407053 at epoch 59 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00381129 at epoch 60 completed in 6m 55s\n",
      "Loss 0.0038437343633600644 is bigger than Loss 0.0038112879475312575 in the prev epoch \n",
      "Loss = 0.00384373 at epoch 61 completed in 6m 55s\n",
      "Loss 0.003862379507693856 is bigger than Loss 0.0038112879475312575 in the prev epoch \n",
      "Loss = 0.00386238 at epoch 62 completed in 6m 55s\n",
      "Loss 0.003811396879144013 is bigger than Loss 0.0038112879475312575 in the prev epoch \n",
      "Loss = 0.00381140 at epoch 63 completed in 6m 55s\n",
      "Loss 0.003879597189836204 is bigger than Loss 0.0038112879475312575 in the prev epoch \n",
      "Loss = 0.00387960 at epoch 64 completed in 6m 55s\n",
      "Learning rate changed from 0.0004 to 8e-05\n",
      "Loss 0.0038679603960897237 is bigger than Loss 0.0038112879475312575 in the prev epoch \n",
      "Loss = 0.00386796 at epoch 65 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00332006 at epoch 66 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00326003 at epoch 67 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00323662 at epoch 68 completed in 6m 55s\n",
      "Loss 0.0032748188407027297 is bigger than Loss 0.003236616749449499 in the prev epoch \n",
      "Loss = 0.00327482 at epoch 69 completed in 6m 55s\n",
      "Loss 0.003263181045784481 is bigger than Loss 0.003236616749449499 in the prev epoch \n",
      "Loss = 0.00326318 at epoch 70 completed in 6m 55s\n",
      "Loss 0.0032538049331467074 is bigger than Loss 0.003236616749449499 in the prev epoch \n",
      "Loss = 0.00325380 at epoch 71 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00320373 at epoch 72 completed in 6m 55s\n",
      "Loss 0.0032191298274057254 is bigger than Loss 0.0032037263874496722 in the prev epoch \n",
      "Loss = 0.00321913 at epoch 73 completed in 6m 55s\n",
      "Loss 0.003218887912774723 is bigger than Loss 0.0032037263874496722 in the prev epoch \n",
      "Loss = 0.00321889 at epoch 74 completed in 6m 55s\n",
      "Loss 0.0032669234814654497 is bigger than Loss 0.0032037263874496722 in the prev epoch \n",
      "Loss = 0.00326692 at epoch 75 completed in 6m 55s\n",
      "Loss 0.00326502349187753 is bigger than Loss 0.0032037263874496722 in the prev epoch \n",
      "Loss = 0.00326502 at epoch 76 completed in 6m 55s\n",
      "Learning rate changed from 8e-05 to 1.6000000000000003e-05\n",
      "Loss 0.0032745276645624213 is bigger than Loss 0.0032037263874496722 in the prev epoch \n",
      "Loss = 0.00327453 at epoch 77 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00307660 at epoch 78 completed in 6m 55s\n",
      "Loss 0.003096692974546128 is bigger than Loss 0.003076601376091795 in the prev epoch \n",
      "Loss = 0.00309669 at epoch 79 completed in 6m 55s\n",
      "Loss 0.0031278862786878444 is bigger than Loss 0.003076601376091795 in the prev epoch \n",
      "Loss = 0.00312789 at epoch 80 completed in 6m 55s\n",
      "Loss 0.0030945263528930284 is bigger than Loss 0.003076601376091795 in the prev epoch \n",
      "Loss = 0.00309453 at epoch 81 completed in 6m 55s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save the weights\n",
      "Loss = 0.00304860 at epoch 82 completed in 6m 55s\n",
      "Loss 0.0030854899877948445 is bigger than Loss 0.0030486040598313733 in the prev epoch \n",
      "Loss = 0.00308549 at epoch 83 completed in 6m 55s\n",
      "Loss 0.003083773666593646 is bigger than Loss 0.0030486040598313733 in the prev epoch \n",
      "Loss = 0.00308377 at epoch 84 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00304607 at epoch 85 completed in 6m 55s\n",
      "Loss 0.0030943791867632958 is bigger than Loss 0.003046065195729691 in the prev epoch \n",
      "Loss = 0.00309438 at epoch 86 completed in 6m 55s\n",
      "Loss 0.0031201671357931823 is bigger than Loss 0.003046065195729691 in the prev epoch \n",
      "Loss = 0.00312017 at epoch 87 completed in 6m 55s\n",
      "Loss 0.0030669188725629018 is bigger than Loss 0.003046065195729691 in the prev epoch \n",
      "Loss = 0.00306692 at epoch 88 completed in 6m 55s\n",
      "Loss 0.0031255328169624732 is bigger than Loss 0.003046065195729691 in the prev epoch \n",
      "Loss = 0.00312553 at epoch 89 completed in 6m 55s\n",
      "Learning rate changed from 1.6000000000000003e-05 to 3.2000000000000007e-06\n",
      "Loss 0.003055084285858486 is bigger than Loss 0.003046065195729691 in the prev epoch \n",
      "Loss = 0.00305508 at epoch 90 completed in 6m 55s\n",
      "Loss 0.003053357005252369 is bigger than Loss 0.003046065195729691 in the prev epoch \n",
      "Loss = 0.00305336 at epoch 91 completed in 6m 55s\n",
      "Loss 0.003070036036893726 is bigger than Loss 0.003046065195729691 in the prev epoch \n",
      "Loss = 0.00307004 at epoch 92 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00303078 at epoch 93 completed in 6m 55s\n",
      "Loss 0.0030617546423205304 is bigger than Loss 0.003030778529122474 in the prev epoch \n",
      "Loss = 0.00306175 at epoch 94 completed in 6m 55s\n",
      "Loss 0.003047530031097786 is bigger than Loss 0.003030778529122474 in the prev epoch \n",
      "Loss = 0.00304753 at epoch 95 completed in 6m 55s\n",
      "save the weights\n",
      "Loss = 0.00297212 at epoch 96 completed in 6m 55s\n",
      "Loss 0.00303879801050893 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00303880 at epoch 97 completed in 6m 55s\n",
      "Loss 0.0030345045223033854 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00303450 at epoch 98 completed in 6m 55s\n",
      "Loss 0.0030269907017105393 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00302699 at epoch 99 completed in 6m 55s\n",
      "Loss 0.003048636938312225 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00304864 at epoch 100 completed in 6m 55s\n",
      "Learning rate changed from 3.2000000000000007e-06 to 6.400000000000001e-07\n",
      "Loss 0.003070691690248038 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00307069 at epoch 101 completed in 6m 55s\n",
      "Loss 0.003068102780463441 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00306810 at epoch 102 completed in 6m 55s\n",
      "Loss 0.003063496029270548 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00306350 at epoch 103 completed in 6m 55s\n",
      "Loss 0.0030367478277268153 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00303675 at epoch 104 completed in 6m 55s\n",
      "Loss 0.0030559077071292077 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00305591 at epoch 105 completed in 6m 55s\n",
      "Learning rate changed from 6.400000000000001e-07 to 1.2800000000000003e-07\n",
      "Loss 0.00301743096525648 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00301743 at epoch 106 completed in 6m 55s\n",
      "Loss 0.0030440807954541264 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00304408 at epoch 107 completed in 6m 55s\n",
      "Loss 0.003049397511407731 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00304940 at epoch 108 completed in 6m 55s\n",
      "Loss 0.0030184406377375115 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00301844 at epoch 109 completed in 6m 55s\n",
      "Loss 0.0030105299060898157 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00301053 at epoch 110 completed in 6m 55s\n",
      "Learning rate changed from 1.2800000000000003e-07 to 2.5600000000000008e-08\n",
      "Loss 0.0030421547032892693 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00304215 at epoch 111 completed in 6m 55s\n",
      "Loss 0.003004565182141961 is bigger than Loss 0.002972122523800603 in the prev epoch \n",
      "Loss = 0.00300457 at epoch 112 completed in 6m 55s\n"
     ]
    }
   ],
   "source": [
    "#MUST UNCOMMENT BELOW LINE...\n",
    "    \n",
    "net = net.cuda()\n",
    "\n",
    "#loading the model after the weights of epoch50.. to check what loss the model gives if lr is taken as 0.0001\n",
    "optimizer = optim.SGD(net.parameters(), lr=.002, momentum=0.9)\n",
    "\n",
    "#criterion = RMSELoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = EuclideanLoss()\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = CosineLoss()\n",
    "#criterion = torch.nn.MSELoss(size_average=False)\n",
    "criterion = CustomLoss()\n",
    "#criterion = BerhuLoss()\n",
    "#criterion = CosineLoss()\n",
    "criterion.cuda()\n",
    "\n",
    "currepochloss =float('Inf')\n",
    "#epochs, n_examples, i, batch_size, flag = 1,5900, 0, 5, 0\n",
    "epochs, n_examples, i, batch_size, flag = 112, 35000, 0, 40, 0\n",
    "\n",
    "\n",
    "while i != epochs:\n",
    "    since = time.time()\n",
    "    cost, batchloss = 0.0, 0.0\n",
    "    num_batches = n_examples//batch_size\n",
    "    #print num_batches    #indices = np.random.permutation(5600)\n",
    "    #indices = np.random.permutation(3524)\n",
    "    \n",
    "    #indices = np.random.permutation(5900)\n",
    "    indices = np.random.permutation(n_examples)\n",
    "    samplesUnprocessed = np.size(indices)\n",
    "    \n",
    "    #batchwise training starts here...\n",
    "    for k in range(num_batches):\n",
    "        since1 = time.time()\n",
    "       # print(\"bacth number:\"+str(k))\n",
    "        xtrain3 = torch.FloatTensor(batch_size,3,180,320)\n",
    "        ytrain3 = torch.FloatTensor(batch_size,1)\n",
    "        ##validPixel = torch.FloatTensor(batch_size,480,640)\n",
    "        \n",
    "        for ind in range(batch_size):\n",
    "            #ind1 = np.random.randint(0,5599)\n",
    "            ind1 = np.random.randint(0,samplesUnprocessed)\n",
    "            #ind1 = np.random.randint(0,794)\n",
    "            #ind1 = np.random.randint(0,794)            \n",
    "            newxind = indices[ind1]            \n",
    "            xtrain3[ind] = xtrainT[newxind]\n",
    "            ytrain3[ind] = ytrainT[newxind,0,0]\n",
    "            ##validPixel[ind] = imgValidTrain2[newxind]\n",
    "            \n",
    "            #print ytrain3[ind,0,0,0], ytrain2[newxind,0,0,0]\n",
    "            indices = np.delete(indices,ind1)\n",
    "            samplesUnprocessed = samplesUnprocessed - 1\n",
    "        \n",
    "        #start, end = k*batch_size, (k+1)*batch_size\n",
    "        #batchloss = train(model5,criterion, optimizer, xtrain3, ytrain3, validPixel,batch_size)\n",
    "        batchloss = train(net,criterion, optimizer, xtrain3, ytrain3, batch_size)\n",
    "        batch_time = time.time() - since1\n",
    "        #cost += batchloss\n",
    "        cost = (cost*k+batchloss)/(k+1)\n",
    "        #print k,cost\n",
    "        #print(\"No. of samples UnProcessed \"+str(samplesUnprocessed))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    epochloss = cost #/num_batches\n",
    "    \n",
    "    if epochloss < currepochloss:\n",
    "        print('save the weights')\n",
    "        torch.save(net.state_dict(),\"./weights/CustomLoss_new/RES_NET_101_CustomLoss_new_35000_DISTANCE_112_epochs.pth\")\n",
    "        flag = 0\n",
    "        currepochloss = epochloss\n",
    "    else:\n",
    "        flag += 1\n",
    "        \n",
    "        if flag == 5:\n",
    "            for p in optimizer.param_groups:\n",
    "                lr2 = p['lr']\n",
    "            newlr = lr2/5\n",
    "            \n",
    "            if newlr < 1e-15:\n",
    "                print(\"Cant decrease further!!\")\n",
    "                newlr = 1e-15\n",
    "            flag = 0 \n",
    "            optimizer = optim.SGD(net.parameters(), lr=newlr, momentum=0.9)\n",
    "            print(\"Learning rate changed from \"+str(lr2)+\" to \"+str(newlr))\n",
    "            \n",
    "        print(\"Loss \"+str(epochloss)+\" is bigger than Loss \"+str(currepochloss)+\" in the prev epoch \")\n",
    "        \n",
    "    print('Loss = {:.8f} at epoch {:d} completed in {:.0f}m {:.0f}s'.format(epochloss,(i+1),(time_elapsed//60),(time_elapsed%60)))\n",
    "    i += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrainT = xtrainT.cpu()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5600000000000008e-08\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for params in optimizer.param_groups:\n",
    "    print(params['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = net.cuda()\n",
    "net.load_state_dict(torch.load(\"./weights/CustomLoss_new/RES_NET_101_CustomLoss_new_35000_DISTANCE_112_epochs.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finalpred size is --->  torch.Size([600, 1])\n",
      "num of batches ---> 30\n"
     ]
    }
   ],
   "source": [
    "#testing of the architecture...\n",
    "num_batches = 0\n",
    "#6 evenly divides the test batch size..\n",
    "test_batch_size = 20\n",
    "n_examples = 600\n",
    "#finalpred = Variable(torch.zeros((n_examples,3,120,160)))\n",
    "finalpred = Variable(torch.zeros((n_examples,1)))\n",
    "print(\"finalpred size is ---> \", finalpred.size())\n",
    "\n",
    "num_batches = n_examples//test_batch_size\n",
    "print(\"num of batches --->\", num_batches)\n",
    "for k in range(num_batches):\n",
    "    start, end = k*test_batch_size, (k+1)*test_batch_size\n",
    "    output = net.forward(Variable(xtestT[start:end], volatile=True).cuda())\n",
    "    finalpred[start:end] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = finalpred.data.numpy()\n",
    "print(data1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600])\n",
      "MSElossRad==0.0019378810490094445 ABSlossRad==0.025634534458319345 RELlossRad0.020585536019446713\n",
      "MSElossDeg==0.11103240530662581 ABSlossDeg==1.4687506342443764 RELlossDeg1.1794643329288332\n"
     ]
    }
   ],
   "source": [
    "#------------------------Angle----------------\n",
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,0,0])\n",
    "dif1 = torch.abs((finalpred.data[:,0]-ytestT[:,0,0])/ytestT[:,0,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "MSElossRad = torch.mean(torch.pow(dif,2))\n",
    "ABSlossRad = torch.mean(dif)\n",
    "RELlossRad = torch.mean(dif1)\n",
    "MSElossDeg = MSElossRad*(180/np.pi)\n",
    "ABSlossDeg = ABSlossRad*(180/np.pi)\n",
    "RELlossDeg = RELlossRad*(180/np.pi)\n",
    "print(\"MSElossRad==\"+str(MSElossRad),\"ABSlossRad==\"+str(ABSlossRad),\"RELlossRad\"+str(RELlossRad))\n",
    "print(\"MSElossDeg==\"+str(MSElossDeg),\"ABSlossDeg==\"+str(ABSlossDeg),\"RELlossDeg\"+str(RELlossDeg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------DISTANCE------------------------\n",
    "dif = torch.abs(finalpred.data[:,0]-ytestT[:,0,0])\n",
    "dif1 = torch.abs((finalpred.data[:,0]-ytestT[:,0,0])/ytestT[:,0,0])\n",
    "print(dif.size())\n",
    "#np.savetxt(\"diff.csv\", dif.numpy(), delimiter=\",\")\n",
    "MSElossNor = torch.mean(torch.pow(dif,2))\n",
    "ABSlossNor = torch.mean(dif)\n",
    "RELlossNor = torch.mean(dif1)\n",
    "MSEloss = MSElossNor*320\n",
    "ABSloss = ABSlossNor*320\n",
    "RELloss = RELlossNor*320\n",
    "print(\"MSElossNor==\"+str(MSElossNor),\"ABSlossNor==\"+str(ABSlossNor),\"RELlossNor\"+str(RELlossNor))\n",
    "print(\"MSEloss==\"+str(MSEloss),\"ABSloss==\"+str(ABSloss),\"RELloss\"+str(RELloss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(finalpred.size())\n",
    "print(ytestT.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "ind = 143\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0,0]*(180/np.pi)))\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Pred angle===\"+str(finalpred.data[ind,0]*(180/np.pi)))\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "\n",
    "\n",
    "#print(finalpred.data[n,0]*(180/np.pi))\n",
    "#print(ytestT[ind,0,0]*(180/np.pi))\n",
    "print(ytestT[ind,0,0]*(180/np.pi)-finalpred.data[ind,0]*(180/np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(ABSlossRad*(180/np.pi))\n",
    "print(MSEloss*(180/np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(ytestT[:,0]*(180/np.pi))\n",
    "#print(ytestT[:,0,2]*(180/np.pi))\n",
    "a = ytestT[:,0,0]*(180/np.pi)\n",
    "print(a.size())\n",
    "np.savetxt(\"test.csv\", a.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a = np.asarray([ [1,2,3], [4,5,6], [7,8,9] ])\n",
    "#np.savetxt(\"foo.csv\", a, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(finalpred.data[:,0]*(180/np.pi))\n",
    "print(finalpred.data[:,0]*(180/np.pi))\n",
    "b = finalpred.data[:,0]*(180/np.pi)\n",
    "c=torch.abs(ytestT[:,0,0]*(180/np.pi)- finalpred.data[:,0]*(180/np.pi))\n",
    "np.savetxt(\"pred.csv\", b.numpy(), delimiter=\",\")\n",
    "\n",
    "np.savetxt(\"diff.csv\", c.numpy(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "finalpred.data[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from scipy.misc import imresize, imread, imshow\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['image.interpolation'] = 'none'\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_figheight(9)\n",
    "fig.set_figwidth(9)\n",
    "import cv2\n",
    "\n",
    "test = cv2.imread(\"./Test_Net_image/4.JPG\")\n",
    "print(test.shape)\n",
    "test = imresize(test,(180,320,3))\n",
    "#imshow(test)\n",
    "test = test.transpose(2,0,1)\n",
    "test = np.reshape(test,(1,3,180,320))\n",
    "test = test.astype(np.float32)\n",
    "testPT = torch.from_numpy(test).float()\n",
    "testPT = batch_rgb_to_bgr(testPT)\n",
    "testPT = torch.div(testPT,255.0)\n",
    "mn = [0.406,0.456,0.485]\n",
    "sd = [0.225,0.224,0.229]\n",
    "norm = Normalize(mn,sd)\n",
    "testPT = norm(testPT)\n",
    "'''\n",
    "ind = 2000\n",
    "testPT = xtestT[ind]\n",
    "print(\"Actual angle===\"+str(ytestT[ind,0]*(180/np.pi)), ytestT[ind,0])\n",
    "testPT = testPT.view(1,3,180,320)\n",
    "#'''\n",
    "test_pred = net.forward(Variable(testPT, volatile=True).cuda())\n",
    "print(\"Angle===\"+str(test_pred.data[0,0]*(180/np.pi)), test_pred.data[0,0])\n",
    "testx = testPT.numpy()\n",
    "testx = np.reshape(testx,(3,180,320))\n",
    "testx = testx.transpose(1,2,0)\n",
    "testx = imresize(testx,(180,320,3))\n",
    "#imshow(testx)\n",
    "scipy.misc.imsave('test.png', testx)\n",
    "a=fig.add_subplot(1,2,1)\n",
    "imgplot = plt.imshow(testx)\n",
    "a.set_title('Input')\n",
    "a.axes.get_xaxis().set_visible(False)\n",
    "a.axes.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
